{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2fdb88b"
   },
   "source": [
    "# Task_7:  customer feedback analysis(NLP + Clustering)\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "*   Load and explore the customer feedback data.\n",
    "*   Clean and preprocess the raw text data for analysis.\n",
    "*   Perform sentiment analysis using multiple approaches (lexicon-based, machine learning, and deep learning).\n",
    "*   Extract relevant features from the processed text.\n",
    "*   Apply clustering techniques to group similar feedback entries based on their text content.\n",
    "*   Analyze the sentiment distribution within the identified clusters.\n",
    "*   Visualize key terms associated with sentiment-specific clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTB9aAZ342Kx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56e72f4b",
    "outputId": "16d9985f-70a9-473d-daf1-ad14ad073e30"
   },
   "outputs": [],
   "source": [
    "%pip install spacy nltk\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3KGLDLtUi9t"
   },
   "source": [
    "#   **Data Loading and Initial Exploration:**\n",
    "    *   Loaded the dataset from the CSV file.\n",
    "    *   Initial checks revealed the shape of the dataset, column names, and the presence of missing values and duplicates.\n",
    "    *   Identified that the 'Confidence Score' column was loaded as an object and needed conversion.\n",
    "    *   Observed missing values in several columns and a significant number of duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Pho6uxsC5Lt-",
    "outputId": "c575ed2c-ea4e-48d4-e40e-9eac3ab1ced4"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment-analysis.csv', delimiter=',')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "1hm3bdcj5Dn2",
    "outputId": "d68a3029-15dd-4bb0-9dc7-c55d486e2476"
   },
   "outputs": [],
   "source": [
    "#Load CSV\n",
    "file_path = '/content/sentiment-analysis.csv'\n",
    "data = []\n",
    "header = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    if lines:\n",
    "        # Extract header from the first line and clean it\n",
    "        header_line = lines[0].strip()\n",
    "        # Split by comma and strip leading/trailing whitespace and quotes\n",
    "        header = [col.strip().strip('\"') for col in header_line.split(',')]\n",
    "\n",
    "        # Process data rows\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "            if line: # Avoid processing empty line\n",
    "                row_data = [item.strip().strip('\"') for item in line.split(',')]\n",
    "                # Ensure row has the same number of columns as header, pad with None if needed\n",
    "                while len(row_data) < len(header):\n",
    "                    row_data.append(None)\n",
    "                data.append(row_data[:len(header)]) # Truncate if more columns than header\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, columns=header)\n",
    "\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j2GVcYc_970s",
    "outputId": "a0d40fc6-0b78-4b5a-8ca6-da749a248ba2"
   },
   "outputs": [],
   "source": [
    "# Remove leading/trailing whitespace from column names to avoid KeyErrors\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(f\"\\n Dataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TG1vplVU-C6r"
   },
   "outputs": [],
   "source": [
    "# Attempt to convert 'Date/Time' to datetime, coercing errors\n",
    "# Check if 'Date/Time' column exists before attempting conversion\n",
    "if 'Date/Time' in df.columns:\n",
    "    df['Date/Time'] = pd.to_datetime(df['Date/Time'], errors='coerce')\n",
    "else:\n",
    "    print(\"Warning: 'Date/Time' column not found after loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duTmHwOp-K-C",
    "outputId": "f19c39fe-d926-459f-c939-841f93b0bf98"
   },
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "JXn-uYI05ah8",
    "outputId": "e743198a-b653-416b-c9f0-35ee06ef01e0"
   },
   "outputs": [],
   "source": [
    "display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hRhLu78-_8F",
    "outputId": "ef55008a-ec77-454a-d199-64b2ed050a7d"
   },
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    'Data_Type': df.dtypes\n",
    "})\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"\\n MISSING VALUES ANALYSIS:\")\n",
    "print(missing_data.to_string(index=False))\n",
    "\n",
    "# Duplicate analysis\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"\\n DUPLICATE ROWS: {duplicate_count} ({duplicate_count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uc8d-ofPUmHd"
   },
   "source": [
    "# **Data Visualization (EDA):**\n",
    "\n",
    "    *  Visualized the distribution of sentiment, feedback sources, text length, confidence scores, and top locations.\n",
    "    *   Insights:\n",
    "        *   The dataset has a relatively balanced distribution of positive and negative sentiments.\n",
    "        *   Feedback comes from a variety of sources, with 'Online Store' being the most frequent.\n",
    "        *   Text lengths vary, suggesting a mix of short and longer feedback.\n",
    "        *   Confidence scores are distributed across the range, indicating varying certainty in the sentiment labels.\n",
    "        *   'Sydney' is the most common location for feedback in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaVHmjySvOtv",
    "outputId": "13df99e0-52e0-4a3c-f5c7-25d7e3ed8415"
   },
   "outputs": [],
   "source": [
    "!pip install -U kaleido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "43e45d2c",
    "outputId": "78ac036f-19ec-47b6-9fb1-d85774a24c97"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "labels = sentiment_counts.index\n",
    "sizes = sentiment_counts.values\n",
    "colors = plt.get_cmap('Set2').colors  # Nice qualitative colors\n",
    "\n",
    "# Plot normal pie chart\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.pie(\n",
    "    sizes,\n",
    "    labels=labels,\n",
    "    autopct='%1.1f%%',  # Show percentages\n",
    "    startangle=90,\n",
    "    colors=colors\n",
    ")\n",
    "\n",
    "ax.set_title(\"Sentiment Distribution\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "63f2561b",
    "outputId": "c44277c3-5d9d-4ca4-9640-0ffc9e8da5c6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data\n",
    "source_counts = df['Source'].value_counts().reset_index()\n",
    "source_counts.columns = ['Source', 'Count']\n",
    "\n",
    "# Plot horizontal bar chart\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(\n",
    "    data=source_counts,\n",
    "    y='Source',\n",
    "    x='Count',\n",
    "    palette='Set2'\n",
    ")\n",
    "\n",
    "plt.title(\"Feedback Source Distribution\", fontsize=16)\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Source\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "0289a8d2",
    "outputId": "63c899f7-aa25-4d3a-a962-8e637c387c4e"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Add a new column for text length\n",
    "df['text_length'] = df['Text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(df['text_length'], bins=50, color='teal', kde=False)\n",
    "\n",
    "plt.title(\"Text Length Distribution\", fontsize=16)\n",
    "plt.xlabel(\"Character Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "32261c1a",
    "outputId": "99fff84f-3289-4bb9-f542-97d8fe2b34e9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure 'Confidence Score' is numeric and drop missing values\n",
    "df_cleaned_confidence = df.dropna(subset=['Confidence Score']).copy()\n",
    "df_cleaned_confidence['Confidence Score'] = pd.to_numeric(df_cleaned_confidence['Confidence Score'], errors='coerce')\n",
    "df_cleaned_confidence.dropna(subset=['Confidence Score'], inplace=True)\n",
    "\n",
    "# Create figure with histogram and boxplot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,8), gridspec_kw={'height_ratios':[4,1]})\n",
    "\n",
    "# Histogram\n",
    "sns.histplot(df_cleaned_confidence['Confidence Score'], bins=30, color='coral', kde=False, ax=ax[0])\n",
    "ax[0].set_title(\"Confidence Score Distribution\", fontsize=16)\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Boxplot on the margin\n",
    "sns.boxplot(x=df_cleaned_confidence['Confidence Score'], color='coral', ax=ax[1])\n",
    "ax[1].set_xlabel(\"Confidence Score\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "ax[1].set_yticks([])  # Hide y-axis for boxplot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "864ff346",
    "outputId": "a6b90f1d-c513-4809-9f9c-07285c76b28f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare top 10 locations\n",
    "top_locations = df['Location'].value_counts().head(10).reset_index()\n",
    "top_locations.columns = ['Location', 'Count']\n",
    "\n",
    "# Plot horizontal bar chart\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(\n",
    "    data=top_locations,\n",
    "    y='Location',\n",
    "    x='Count',\n",
    "    palette='tab10'  # Similar qualitative palette to Plotly\n",
    ")\n",
    "\n",
    "plt.title(\"Top 10 Feedback Locations\", fontsize=16)\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Location\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "35ade7ff",
    "outputId": "124ab625-81a2-46f0-d98b-5a6fde18e01b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot boxplot\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x='Sentiment',\n",
    "    y='text_length',\n",
    "    palette='Set2'\n",
    ")\n",
    "\n",
    "plt.title(\"Text Length Distribution by Sentiment\", fontsize=16)\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Text Length\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "94fcb4d3",
    "outputId": "53a478c4-a39e-4eb9-c958-f4caa4942d34"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure 'Date/Time' is datetime and drop missing values\n",
    "df_cleaned_datetime = df.dropna(subset=['Date/Time']).copy()\n",
    "df_cleaned_datetime['Date/Time'] = pd.to_datetime(df_cleaned_datetime['Date/Time'])\n",
    "\n",
    "# Resample to daily counts\n",
    "daily_counts = df_cleaned_datetime.set_index('Date/Time').resample('D').size().reset_index(name='Count')\n",
    "\n",
    "# Plot line chart\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=daily_counts, x='Date/Time', y='Count', color='purple')\n",
    "\n",
    "plt.title(\"Feedback Volume Over Time\", fontsize=16)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "640b2c24",
    "outputId": "aa06c2a8-e4c5-40de-d687-7e5577457093"
   },
   "outputs": [],
   "source": [
    "# Get the top 10 locations\n",
    "top_10_locations = df['Location'].value_counts().head(10).index\n",
    "\n",
    "# Filter the DataFrame to include only the top 10 locations\n",
    "df_top_locations = df[df['Location'].isin(top_10_locations)]\n",
    "\n",
    "# Group by Location and Sentiment and count the occurrences\n",
    "location_sentiment_counts = df_top_locations.groupby(['Location', 'Sentiment']).size().reset_index(name='Count')\n",
    "\n",
    "# Pivot data for grouped bar chart\n",
    "pivot_df = location_sentiment_counts.pivot(index='Location', columns='Sentiment', values='Count').fillna(0)\n",
    "\n",
    "# Plot grouped horizontal bar chart\n",
    "pivot_df.plot(kind='barh',\n",
    "              stacked=False,\n",
    "              figsize=(10,6),\n",
    "              color=sns.color_palette(\"Set2\", n_colors=len(pivot_df.columns)))\n",
    "\n",
    "plt.title(\"Sentiment Distribution by Top 10 Locations\", fontsize=16)\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Location\")\n",
    "plt.legend(title='Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "RdsQpKNuC5vB",
    "outputId": "ca9da8f9-46a6-4790-a7a4-268f7dc621f6"
   },
   "outputs": [],
   "source": [
    "# Filter out sentiments with no valid text length data before plotting\n",
    "sentiment_data = {sent: df[df['Sentiment'] == sent]['text_length'].dropna().values\n",
    "                  for sent in df['Sentiment'].unique()}\n",
    "valid_sentiments = [sent for sent, data in sentiment_data.items() if len(data) > 0]\n",
    "valid_data = [sentiment_data[sent] for sent in valid_sentiments]\n",
    "\n",
    "if valid_data: # Check if there is any valid data to plot\n",
    "    fig, ax5 = plt.subplots(figsize=(8, 6)) # Create a new figure and axes for this plot\n",
    "    parts = ax5.violinplot(valid_data,\n",
    "                           positions=range(len(valid_sentiments)),\n",
    "                           showmeans=True, showextrema=True,\n",
    "                           widths=0.6) # Adjust width for better spacing\n",
    "\n",
    "    # Customize violin plot colors\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "    for i, pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor(colors[i % len(colors)])\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.8)\n",
    "\n",
    "    # Customize mean and extrema lines\n",
    "    for part in ('cbars','cmins','cmaxes'):\n",
    "        parts[part].set_edgecolor('gray')\n",
    "        parts[part].set_linewidth(1.5)\n",
    "\n",
    "    parts['cmeans'].set_edgecolor('red')\n",
    "    parts['cmeans'].set_linewidth(2)\n",
    "\n",
    "\n",
    "    ax5.set_title('Text Length Distribution by Sentiment', fontsize=14, fontweight='bold')\n",
    "    ax5.set_xlabel('Sentiment', fontsize=12)\n",
    "    ax5.set_ylabel('Text Length', fontsize=12)\n",
    "    ax5.set_xticks(range(len(valid_sentiments)))\n",
    "    ax5.set_xticklabels(valid_sentiments)\n",
    "    ax5.grid(axis='y', alpha=0.7, linestyle='--') # Add horizontal grid lines\n",
    "    ax5.grid(axis='x', alpha=0.3)\n",
    "\n",
    "\n",
    "else:\n",
    "    fig, ax5 = plt.subplots(figsize=(8, 6)) # Create a new figure and axes even if no data\n",
    "    ax5.text(0.5, 0.5, \"No valid data for Violin Plot\", horizontalalignment='center', verticalalignment='center', transform=ax5.transAxes)\n",
    "    ax5.set_title('Text Length Distribution by Sentiment', fontsize=14, fontweight='bold')\n",
    "    ax5.set_xlabel('Sentiment', fontsize=12)\n",
    "    ax5.set_ylabel('Text Length', fontsize=12)\n",
    "\n",
    "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "8ffd49be",
    "outputId": "58f081ac-554b-4178-c7d5-d31e31a221ec"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Group by Source and Sentiment and count occurrences\n",
    "source_sentiment_counts = df.groupby(['Source', 'Sentiment']).size().reset_index(name='Count')\n",
    "\n",
    "# Pivot data for stacked bar chart\n",
    "pivot_df = source_sentiment_counts.pivot(index='Source', columns='Sentiment', values='Count').fillna(0)\n",
    "\n",
    "# Define colors for sentiments\n",
    "colors = {'Positive':'green', 'Negative':'red', 'Neutral':'gray'}\n",
    "sentiment_colors = [colors[col] for col in pivot_df.columns]\n",
    "\n",
    "# Plot stacked bar chart\n",
    "pivot_df.plot(kind='bar',\n",
    "              stacked=True,\n",
    "              figsize=(12,6),\n",
    "              color=sentiment_colors)\n",
    "\n",
    "plt.title(\"Sentiment Distribution within Each Source\", fontsize=16)\n",
    "plt.xlabel(\"Source\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pa8zdLKTEmN9",
    "outputId": "0949a72b-065a-45f5-8d66-012592319b80"
   },
   "outputs": [],
   "source": [
    "# Store original shape\n",
    "original_shape = df.shape\n",
    "print(f\"\\n Original dataset shape: {original_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhQaTk07EqHz"
   },
   "outputs": [],
   "source": [
    "# Remove rows with null Text\n",
    "null_text_count = df['Text'].isnull().sum()\n",
    "df_clean = df[df['Text'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GhncaWPIEzCq",
    "outputId": "3eaa9bf0-7a97-42b4-c681-89ac66018d18"
   },
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "duplicate_count = df_clean.duplicated(subset=['Text']).sum()\n",
    "df_clean = df_clean.drop_duplicates(subset=['Text'], keep='first').copy()\n",
    "print(f\" Removed {duplicate_count} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6KfIrlKXE9UD",
    "outputId": "6e472533-9447-46f9-ba81-9be6089ad1dc"
   },
   "outputs": [],
   "source": [
    "# Remove empty strings\n",
    "empty_text_count = (df_clean['Text'].str.strip() == '').sum()\n",
    "df_clean = df_clean[df_clean['Text'].str.strip() != ''].copy()\n",
    "print(f\" Removed {empty_text_count} rows with empty Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6oEdYYfEN63",
    "outputId": "95db7770-a149-431d-9382-97ce88ae7dff"
   },
   "outputs": [],
   "source": [
    "# Final shape\n",
    "final_shape = df_clean.shape\n",
    "print(f\"\\n Cleaned dataset shape: {final_shape}\")\n",
    "print(f\" Total rows removed: {original_shape[0] - final_shape[0]}\")\n",
    "print(f\" Data retention rate: {final_shape[0]/original_shape[0]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "_XUoKOIjFO03",
    "outputId": "055db2a7-1feb-428d-a452-9577d9cf7556"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7zwLf3KVRi2"
   },
   "source": [
    "\n",
    "#    **Text Data Cleaning and Preprocessing:**\n",
    "    *   Performed several cleaning steps on the 'Text' column, including converting to lowercase, removing URLs, emails, mentions, hashtags, numbers, punctuation, and extra whitespaces.\n",
    "    *   Applied tokenization, stopword removal, and lemmatization to prepare the text for analysis.\n",
    "    *   Calculated processed text statistics (word count, unique word count, lexical diversity).\n",
    "    *   Insights:\n",
    "        *   The cleaning process successfully standardized the text and removed noise.\n",
    "        *   Tokenization and lemmatization created a cleaner set of terms for analysis.\n",
    "        *   Processed text statistics provide a quantitative view of the text complexity after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfeMJtNqGWuB",
    "outputId": "cfb7bddf-3bb2-4c5d-fe0b-f7d2822277eb"
   },
   "outputs": [],
   "source": [
    "# Initialize NLP tools\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Extended stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = {'im', 'ive', 'thats', 'dont', 'didnt', 'cant', 'wont', 'wouldnt'}\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEXT PREPROCESSING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create preprocessing columns\n",
    "df_clean['text_lowercase'] = df_clean['Text'].str.lower()\n",
    "print(\" Step 1: Converted to lowercase\")\n",
    "\n",
    "# Remove URLs\n",
    "df_clean['text_no_urls'] = df_clean['text_lowercase'].str.replace(r'http\\S+|www\\S+|https\\S+', '', regex=True)\n",
    "print(\" Step 2: Removed URLs\")\n",
    "\n",
    "# Remove email addresses\n",
    "df_clean['text_no_emails'] = df_clean['text_no_urls'].str.replace(r'\\S+@\\S+', '', regex=True)\n",
    "print(\" Step 3: Removed email addresses\")\n",
    "\n",
    "# Remove mentions and hashtags\n",
    "df_clean['text_no_mentions'] = df_clean['text_no_emails'].str.replace(r'@\\w+|#\\w+', '', regex=True)\n",
    "print(\" Step 4: Removed mentions and hashtags\")\n",
    "\n",
    "# Remove numbers\n",
    "df_clean['text_no_numbers'] = df_clean['text_no_mentions'].str.replace(r'\\d+', '', regex=True)\n",
    "print(\" Step 5: Removed numbers\")\n",
    "\n",
    "# Remove punctuation and special characters\n",
    "df_clean['text_no_punct'] = df_clean['text_no_numbers'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
    "print(\" Step 6: Removed punctuation\")\n",
    "\n",
    "# Remove extra whitespaces\n",
    "df_clean['text_clean'] = df_clean['text_no_punct'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "print(\" Step 7: Removed extra whitespaces\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "sample_idx = 0\n",
    "print(f\"\\nOriginal: {df_clean.loc[sample_idx, 'Text']}\")\n",
    "print(f\"\\nCleaned: {df_clean.loc[sample_idx, 'text_clean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LodpyAcEHCHD",
    "outputId": "a6cc23b0-c167-4322-b986-cc44439778fb"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  ADVANCED TEXT PREPROCESSING - PART 2 (TOKENIZATION & NORMALIZATION)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Apply tokenization, stopword removal, and lemmatization to the cleaned text.\n",
    "This completes the text preprocessing pipeline.\n",
    "\"\"\"\n",
    "# NLTK's tokenizer is required for this step\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# Download the missing punkt_tab resource\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "# Define the function for token-level processing\n",
    "def normalize_tokens(text):\n",
    "    # Tokenize the text into individual words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Lemmatize tokens and remove stopwords simultaneously for efficiency\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Join the processed tokens back into a single string\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOKEN NORMALIZATION PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply the normalization function to the 'text_clean' column\n",
    "df_clean['text_processed'] = df_clean['text_clean'].apply(normalize_tokens)\n",
    "print(\" Step 8: Tokenized text\")\n",
    "print(\" Step 9: Removed stopwords\")\n",
    "print(\" Step 10: Lemmatized tokens\")\n",
    "\n",
    "# Display the final transformation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL PREPROCESSING EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "sample_idx = 0 # Using the same sample index\n",
    "print(f\"\\nOriginal: {df_clean.loc[sample_idx, 'Text']}\")\n",
    "print(f\"\\nCleaned (String-level): {df_clean.loc[sample_idx, 'text_clean']}\")\n",
    "print(f\"\\nFinal Processed (Token-level): {df_clean.loc[sample_idx, 'text_processed']}\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"Text preprocessing is complete.\")\n",
    "print(\"The 'text_processed' column is now ready for the next phase: Feature Extraction.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "referenced_widgets": [
      "6b06e146338245dbab58abb26144725b",
      "c472fda8fd4b48959c3dace18785d92c",
      "0cc0052310cc4228a02f6c5492ea6048",
      "5d7a1d06109a4a95bcd2c336581c2a0e",
      "e6dccc081b4c4bc389ac1ccf79a02dbc",
      "707efc1df5194a7f8aa7d4978156ecb5",
      "16d4cacd0b31417693fafe828cfcaec3",
      "fc2f99774725471f92beee01e5cd5a99",
      "cb548fdc81624934b13794c6d453f776",
      "35f4d99ed6a44822951e012d2ded08b5",
      "1d2da624e1bb46a18ea7790f3b98c38e",
      "c4d098c91f804b3b883a84d91f980646",
      "e495110fa77e4d53a97c5e98eb00f5c4",
      "9a5e19f0bbe2432aa169d43510bc6ccb",
      "b9e48ae1560841229eb2873ac42e0c82",
      "8b7c511264354cbdb8b08329a0c19436",
      "a0b62183436842258e562a51b0711054",
      "273e9168a2814ae2932a3554ead19593",
      "41bf14d244be414e95f94675f74d2938",
      "215d1c76413d4e29a1c39748a8829bfb",
      "29983e9511b046c2b5c07c4d12487b80",
      "09b2dfbfd8c843bf94ef9582b8bb113a",
      "7b8a679921b24c53981ef39d59b7e122",
      "87889309a33946f69f0bf4f18c86acb1",
      "db17893736274dd49509f0ef2c0869d7",
      "f19d5a46b8d948bcb32d2902c41ad84a",
      "dd6d27e5b41245f189bed53be70f50fa",
      "d5e711fb6f15462db16e9a9432c7953c",
      "03671158fd374cdeb7427aafdafbaff5",
      "f4b1911a5d624cb5bf38bdd184b2a684",
      "274c5259e7044441b207678bee29c324",
      "39a78fd4d52d4ddf87d1d5e135cd1d8b",
      "8dea507b807f406c87cd2d3873d456d4",
      "f0a6c6c3a45744dfb0247bcc09420554",
      "e51cfb8532244b279fe36a76c185e46d",
      "d1f3bc158d5647368934f2851ce7175f",
      "4566c769a52f41059e950db0c68a3dff",
      "382f55da6f37471282a49f4354941242",
      "4bd7faf540be4eb9a957296075ae6ef2",
      "f880cfe7b85b4fd5ae86038e44a6ce33",
      "2e49621162704e198390305417febbaf",
      "d719b148de364264b0818fe819914242",
      "cb1105e70d76463fb71c0a829c8a4898",
      "f1fcc0fe4f0d424f938a50f2ef00bebe"
     ]
    },
    "id": "UbIMUPJXIYjh",
    "outputId": "24cd4d9e-0617-4c57-a10d-a6e93d3fd28d"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  ADVANCED TEXT PREPROCESSING - PART 2 (TOKENIZATION & LEMMATIZATION)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Tokenization, stopword removal, and lemmatization\n",
    "\"\"\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Tokenization\n",
    "tqdm.pandas(desc=\"Tokenizing\")\n",
    "df_clean['tokens'] = df_clean['text_clean'].progress_apply(word_tokenize)\n",
    "print(\" Tokenization completed\")\n",
    "\n",
    "# Remove stopwords\n",
    "tqdm.pandas(desc=\"Removing stopwords\")\n",
    "df_clean['tokens_no_stop'] = df_clean['tokens'].progress_apply(\n",
    "    lambda tokens: [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    ")\n",
    "print(\" Stopword removal completed\")\n",
    "\n",
    "# Lemmatization\n",
    "tqdm.pandas(desc=\"Lemmatizing\")\n",
    "df_clean['tokens_lemmatized'] = df_clean['tokens_no_stop'].progress_apply(\n",
    "    lambda tokens: [lemmatizer.lemmatize(word) for word in tokens]\n",
    ")\n",
    "print(\" Lemmatization completed\")\n",
    "\n",
    "# Stemming (alternative approach)\n",
    "tqdm.pandas(desc=\"Stemming\")\n",
    "df_clean['tokens_stemmed'] = df_clean['tokens_no_stop'].progress_apply(\n",
    "    lambda tokens: [stemmer.stem(word) for word in tokens]\n",
    ")\n",
    "print(\" Stemming completed\")\n",
    "\n",
    "# Join tokens back to text\n",
    "df_clean['text_processed'] = df_clean['tokens_lemmatized'].apply(lambda x: ' '.join(x))\n",
    "df_clean['text_stemmed'] = df_clean['tokens_stemmed'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Calculate processed text statistics\n",
    "df_clean['word_count'] = df_clean['tokens_lemmatized'].apply(len)\n",
    "df_clean['unique_word_count'] = df_clean['tokens_lemmatized'].apply(lambda x: len(set(x)))\n",
    "df_clean['lexical_diversity'] = df_clean['unique_word_count'] / (df_clean['word_count'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNAPe8TOVoKs"
   },
   "source": [
    "\n",
    "#    **Sentiment Analysis (Lexicon-Based - VADER):**\n",
    "    *   Applied VADER, a lexicon-based tool, to the processed text to get sentiment scores and labels.\n",
    "    *   Insights:\n",
    "        *   VADER provided sentiment labels and scores based on its pre-defined lexicon.\n",
    "        *   The distribution of VADER sentiments can be compared to the original sentiment labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "id": "a856063e",
    "outputId": "50f97576-5fb3-4160-9175-2e062df8b618"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  LEXICON-BASED SENTIMENT ANALYSIS (VADER)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Apply a lexicon-based sentiment analysis tool (VADER) to the processed text.\n",
    "VADER ( Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and\n",
    "rule-based sentiment analysis tool that is specifically attuned to sentiments\n",
    "expressed in social media.\n",
    "\"\"\"\n",
    "# Install VADER if not already installed\n",
    "try:\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "except ImportError:\n",
    "    %pip install nltk\n",
    "    import nltk\n",
    "\n",
    "# Download the VADER lexicon\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Now import SentimentIntensityAnalyzer after ensuring nltk and lexicon are available\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SENTIMENT ANALYSIS: LEXICON-BASED (VADER)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize VADER sentiment intensity analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get VADER sentiment score and label\n",
    "def vader_sentiment(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None, None # Handle non-string input\n",
    "\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    # Determine sentiment label based on compound score\n",
    "    if scores['compound'] >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    return sentiment, scores['compound']\n",
    "\n",
    "# Apply VADER to the processed text and create new columns\n",
    "# Use df_clean as it contains the processed text\n",
    "df_clean['vader_sentiment_label'], df_clean['vader_sentiment_score'] = zip(*df_clean['text_processed'].apply(vader_sentiment))\n",
    "\n",
    "print(\"✓ VADER sentiment analysis completed.\")\n",
    "\n",
    "# Display the head of the DataFrame with new VADER columns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VADER SENTIMENT ANALYSIS RESULTS (PREVIEW)\")\n",
    "print(\"=\"*80)\n",
    "display(df_clean[['Text', 'text_processed', 'Sentiment', 'vader_sentiment_label', 'vader_sentiment_score']].head())\n",
    "\n",
    "# Display sentiment distribution from VADER\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VADER SENTIMENT DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "display(df_clean['vader_sentiment_label'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Lexicon-based sentiment analysis is complete. We can now proceed to Machine Learning based approaches.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riec5NTSVtEq"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sbtbO_nVzZY"
   },
   "source": [
    "\n",
    "#   **Sentiment Analysis (Deep Learning Approach - Simple ANN):**\n",
    "    *   Implemented and trained a simple Artificial Neural Network for sentiment classification.\n",
    "    *   Evaluated the model's performance.\n",
    "    *   Insights:\n",
    "        *   Deep learning models can also be applied to sentiment analysis, potentially capturing more complex patterns.\n",
    "        *   Performance metrics allow comparison with the traditional ML approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5c05899d",
    "outputId": "c67e07f9-8b4e-4e7d-c5e5-892c11f6bc38"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  DEEP LEARNING SENTIMENT ANALYSIS (SIMPLE ANN)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Implement a basic Artificial Neural Network (ANN) for sentiment analysis\n",
    "using pre-trained word embeddings or by learning embeddings from scratch.\n",
    "This serves as an introduction to Deep Learning for this task.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SENTIMENT ANALYSIS: DEEP LEARNING APPROACH (SIMPLE ANN)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- 1. Data Preparation for Deep Learning ---\n",
    "# Use the cleaned data and the original Sentiment labels\n",
    "df_dl = df_clean.dropna(subset=['Sentiment']).copy()\n",
    "\n",
    "# Encode the sentiment labels (Positive, Negative, Neutral) into numerical form\n",
    "# Use the existing sentiment_mapping for consistency if available, otherwise create one\n",
    "if 'sentiment_mapping' not in locals():\n",
    "     sentiment_mapping = {'Positive': 1, 'Negative': 0, 'Neutral': 2}\n",
    "     print(\"Note: Creating new sentiment_mapping for DL as it was not found.\")\n",
    "\n",
    "# Filter out sentiments that are not in the mapping if necessary\n",
    "df_dl = df_dl[df_dl['Sentiment'].isin(sentiment_mapping.keys())].copy()\n",
    "\n",
    "\n",
    "y_text = df_dl['Sentiment'].values\n",
    "# Ensure all labels in y_text are in sentiment_mapping\n",
    "y_encoded = np.array([sentiment_mapping[label] for label in y_text])\n",
    "\n",
    "\n",
    "# Tokenize the text data\n",
    "# Use the 'text_processed' column for consistency with ML\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\") # Limit vocabulary size\n",
    "tokenizer.fit_on_texts(df_dl['text_processed'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert texts to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(df_dl['text_processed'])\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "max_length = max([len(x) for x in sequences]) if sequences else 0 # Determine max length, handle empty case\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "print(f\" Vocabulary size: {len(word_index)}\")\n",
    "print(f\" Max sequence length: {max_length}\")\n",
    "print(f\" Padded sequences shape: {padded_sequences.shape}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# Use the same random_state as the ML split for potential comparison consistency\n",
    "X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(\n",
    "    padded_sequences, y_encoded, test_size=0.25, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\n Training data shape (padded): {X_train_dl.shape}\")\n",
    "print(f\" Testing data shape (padded): {X_test_dl.shape}\")\n",
    "print(f\" Training labels shape: {y_train_dl.shape}\")\n",
    "print(f\" Testing labels shape: {y_test_dl.shape}\")\n",
    "\n",
    "# Determine the number of classes for the output layer\n",
    "num_classes = len(sentiment_mapping)\n",
    "\n",
    "\n",
    "# --- 2. Build the Deep Learning Model ---\n",
    "model_dl = Sequential([\n",
    "\n",
    "    Embedding(input_dim=len(word_index) + 1, output_dim=64, input_length=max_length),\n",
    "    GlobalAveragePooling1D(),\n",
    "\n",
    "    # Dense layers: Standard feedforward layers\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5), # Dropout for regularization\n",
    "    Dense(num_classes, activation='softmax') # Output layer with softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_dl.compile(\n",
    "    loss='sparse_categorical_crossentropy', # Suitable for integer encoded labels\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_dl.summary()\n",
    "\n",
    "\n",
    "history = model_dl.fit(\n",
    "    X_train_dl, y_train_dl,\n",
    "    epochs=20, # Number of training epochs\n",
    "    validation_split=0.2, # Percentage of training data to use for validation\n",
    "    verbose=1 # Show training progress\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Deep Learning model training completed.\")\n",
    "\n",
    "\n",
    "# --- 4. Evaluate the Model ---\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model_dl.evaluate(X_test_dl, y_test_dl, verbose=0)\n",
    "print(f\"\\n Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Get predictions for the classification report\n",
    "y_pred_dl = np.argmax(model_dl.predict(X_test_dl), axis=-1)\n",
    "\n",
    "# Decode predicted labels back to sentiment names for evaluation readability\n",
    "# Create a reverse mapping\n",
    "reverse_sentiment_mapping_dl = {v: k for k, v in sentiment_mapping.items()}\n",
    "y_test_labels_dl = pd.Series(y_test_dl).map(reverse_sentiment_mapping_dl)\n",
    "y_pred_labels_dl = pd.Series(y_pred_dl).map(reverse_sentiment_mapping_dl)\n",
    "\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEEP LEARNING MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels_dl, y_pred_labels_dl))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Deep Learning based sentiment analysis is complete. We can now compare the performance of the three models.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5NSQQlKV4PN"
   },
   "source": [
    "\n",
    "#    **Sentiment Analysis (Machine Learning Approach):**\n",
    "    *   Trained a Multinomial Naive Bayes classifier on TF-IDF features of the processed text using the original sentiment labels.\n",
    "    *   Evaluated the model's performance using accuracy and a classification report.\n",
    "    *   Insights:\n",
    "        *   A machine learning model can effectively classify sentiment based on text features.\n",
    "        *   The evaluation metrics (Accuracy, Precision, Recall, F1-score) provide a quantitative measure of the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c2ff961",
    "outputId": "d6a21a2c-a158-462d-9b8e-0dca63862a89"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  MACHINE LEARNING SENTIMENT ANALYSIS\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Implement a machine learning model for sentiment analysis using the existing\n",
    "'Sentiment' column as labels. This involves:\n",
    "1. Data Preparation (Splitting into training and testing sets)\n",
    "2. Feature Extraction (Using TF-IDF or similar on the processed text)\n",
    "3. Model Selection and Training (Using a classifier like Naive Bayes or Logistic Regression)\n",
    "4. Model Evaluation\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SENTIMENT ANALYSIS: MACHINE LEARNING APPROACH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "# Drop rows where 'Sentiment' is missing, as these are needed for training\n",
    "df_ml = df_clean.dropna(subset=['Sentiment']).copy()\n",
    "\n",
    "# Map sentiment labels to numerical values for the model\n",
    "sentiment_mapping = {'Positive': 1, 'Negative': 0, 'Neutral': 2} # Assuming Neutral exists, adjust if needed\n",
    "df_ml['sentiment_encoded'] = df_ml['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# Use the 'text_processed' column which contains cleaned and normalized text\n",
    "X = df_ml['text_processed']\n",
    "y = df_ml['sentiment_encoded']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# Stratify by y to ensure similar sentiment distribution in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\" Training data shape: {X_train.shape}\")\n",
    "print(f\" Testing data shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. Feature Extraction ---\n",
    "# Initialize and fit TF-IDF Vectorizer on the training data\n",
    "# Reuse parameters from the previous TF-IDF step for consistency, but fit on training data\n",
    "tfidf_vectorizer_ml = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.7,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer_ml.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer_ml.transform(X_test) # Transform test data using the fitted vectorizer\n",
    "\n",
    "print(f\"\\n Training data TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "print(f\" Testing data TF-IDF shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "\n",
    "# --- 3. Model Selection and Training ---\n",
    "# Using Multinomial Naive Bayes, a common and effective model for text classification\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"\\n✓ Machine Learning model training completed.\")\n",
    "\n",
    "\n",
    "# --- 4. Model Evaluation ---\n",
    "# Predict sentiment on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Decode predicted labels back to sentiment names for evaluation readability\n",
    "# Create a reverse mapping\n",
    "reverse_sentiment_mapping = {v: k for k, v in sentiment_mapping.items()}\n",
    "y_test_labels = y_test.map(reverse_sentiment_mapping)\n",
    "y_pred_labels = pd.Series(y_pred).map(reverse_sentiment_mapping)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MACHINE LEARNING MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Machine Learning based sentiment analysis is complete. We can now consider Deep Learning or proceed to evaluation.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "81f8d6f7",
    "outputId": "96d0e9fe-031f-472a-bb4c-ddc6449843d7"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#VISUALIZE SENTIMENT MODEL PERFORMANCE COMPARISON\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Visualize the performance metrics (Accuracy and F1-score) of the trained\n",
    "Machine Learning and Deep Learning sentiment analysis models to compare them\n",
    "visually and help in selecting the best model for further analysis.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SENTIMENT MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get classification report dictionaries\n",
    "ml_report = classification_report(y_test_labels, y_pred_labels, output_dict=True)\n",
    "dl_report = classification_report(y_test_labels_dl, y_pred_labels_dl, output_dict=True)\n",
    "\n",
    "# Extract relevant metrics\n",
    "# Using 'weighted avg' F1-score as it's suitable for imbalanced datasets\n",
    "ml_accuracy = accuracy_score(y_test, y_pred)\n",
    "ml_f1 = ml_report['weighted avg']['f1-score']\n",
    "\n",
    "dl_accuracy = accuracy_score(y_test_dl, y_pred_dl)\n",
    "dl_f1 = dl_report['weighted avg']['f1-score']\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "performance_data = {\n",
    "    'Model': ['Machine Learning (Naive Bayes)', 'Deep Learning (Simple ANN)'],\n",
    "    'Accuracy': [ml_accuracy, dl_accuracy],\n",
    "    'F1 Score (Weighted Avg)': [ml_f1, dl_f1]\n",
    "}\n",
    "df_performance = pd.DataFrame(performance_data)\n",
    "\n",
    "# Reshape DataFrame for melting (suitable for seaborn barplot)\n",
    "df_performance_melted = df_performance.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
    "\n",
    "# Create the bar plot\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Score', hue='Metric', data=df_performance_melted, palette='viridis')\n",
    "\n",
    "plt.title('Sentiment Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylim(0, 1) # Scores are between 0 and 1\n",
    "plt.legend(title='Metric')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add text labels for the scores\n",
    "for container in plt.gca().containers:\n",
    "    plt.bar_label(container, fmt='%.3f')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "8a017322",
    "outputId": "80f5f11d-2083-41a1-d519-8b210507808e"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ADD MACHINE LEARNING SENTIMENT PREDICTIONS TO DATAFRAME\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADDING MACHINE LEARNING SENTIMENT PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure df_clean and y_pred_labels (from ML model evaluation) are available\n",
    "if 'df_clean' in locals() and 'y_pred_labels' in locals():\n",
    "    df_ml_for_merge = df_clean.dropna(subset=['Sentiment']).copy()\n",
    "    X_for_merge = df_ml_for_merge['text_processed']\n",
    "    y_for_merge = df_ml_for_merge['Sentiment'] # Use original sentiment for mapping\n",
    "\n",
    "    # Get the indices of the test set used for prediction\n",
    "    X_train_idx, X_test_idx, y_train_idx, y_test_idx = train_test_split(\n",
    "        df_ml_for_merge.index, y_for_merge, test_size=0.25, random_state=42, stratify=y_for_merge\n",
    "    )\n",
    "\n",
    "    # Create a Series of predictions with the test set index\n",
    "    ml_predictions_series = pd.Series(y_pred_labels.values, index=X_test_idx)\n",
    "\n",
    "    # Add a new column for predicted sentiment, initializing with None\n",
    "    df_clean['predicted_sentiment_ml'] = None\n",
    "\n",
    "    # Update the predicted sentiment for the test set rows\n",
    "    df_clean.loc[X_test_idx, 'predicted_sentiment_ml'] = ml_predictions_series\n",
    "\n",
    "    print(\"✓ Machine Learning sentiment predictions added to 'predicted_sentiment_ml' column.\")\n",
    "    print(\"Note: Predictions are only available for the test set used in evaluation.\")\n",
    "\n",
    "    # Display the head of the DataFrame with the new column\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATAFRAME HEAD WITH PREDICTED SENTIMENT\")\n",
    "    print(\"=\"*80)\n",
    "    display(df_clean[['Text', 'Sentiment', 'predicted_sentiment_ml']].head())\n",
    "\n",
    "else:\n",
    "    print(\"Error: Required variables (df_clean, y_pred_labels) not found. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A-v729tWEwA"
   },
   "source": [
    "\n",
    "#   **Feature Extraction (TF-IDF Vectorization):**\n",
    "    *   Converted the processed text into numerical TF-IDF vectors.\n",
    "    *   Insights:\n",
    "        *   TF-IDF represents the importance of terms in documents relative to the corpus, preparing the data for machine learning and clustering algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BI8Re83AJN8-",
    "outputId": "5683fe8e-883b-4b28-c0f8-f2e7842aee8d"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  FEATURE EXTRACTION - TF-IDF VECTORIZATION\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE EXTRACTION: TF-IDF\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.7,\n",
    "    sublinear_tf=True # Apply sublinear TF scaling\n",
    ")\n",
    "\n",
    "# Fit the vectorizer to the processed text and transform it into a matrix\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_clean['text_processed'])\n",
    "\n",
    "print(\"✓ TF-IDF vectorization completed successfully.\")\n",
    "print(f\"Shape of the resulting TF-IDF matrix: {X_tfidf.shape}\")\n",
    "print(f\"This matrix represents {X_tfidf.shape[0]} documents and {X_tfidf.shape[1]} unique word/phrase features.\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"The data is now in a numerical format, ready for clustering.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gTl9GGgWYmU"
   },
   "source": [
    "\n",
    "#    **Dimensionality Reduction (UMAP):**\n",
    "    *   Applied UMAP to reduce the dimensionality of the TF-IDF matrix.\n",
    "    *   Insights:\n",
    "        *   UMAP preserves the local and global structure of the data in a lower-dimensional space, making it suitable for visualization and potentially improving clustering performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4691e8da",
    "outputId": "dd533608-6a52-4c9d-d182-501c3ee1e3b3"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  DIMENSIONALITY REDUCTION (UMAP) FOR CLUSTERING\n",
    "# ============================================================================\n",
    "!pip install umap-learn\n",
    "\n",
    "import umap\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIMENSIONALITY REDUCTION: UMAP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_tfidf (TF-IDF matrix) is available from previous steps\n",
    "if 'X_tfidf' not in locals():\n",
    "    print(\"Error: X_tfidf matrix not found. Please run the TF-IDF feature extraction cell first.\")\n",
    "else:\n",
    "    umap_reducer_clustering = umap.UMAP(n_components=2, random_state=42) # Can adjust n_components\n",
    "    X_umap = umap_reducer_clustering.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ced71ef4",
    "outputId": "e30a791e-e502-4e98-df2b-515bb2b875c2"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# K-MEANS CLUSTERING ON UMAP-REDUCED DATA - FINDING OPTIMAL K (ELBOW METHOD)\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"K-MEANS ON UMAP DATA: DETERMINING OPTIMAL NUMBER OF CLUSTERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate inertia for a range of cluster numbers on the UMAP data\n",
    "wcss_umap = [] # Within-Cluster Sum of Squares on UMAP data\n",
    "k_range_umap = range(2, 16) # Testing from 2 to 15 clusters\n",
    "\n",
    "for i in k_range_umap:\n",
    "    kmeans_umap = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans_umap.fit(X_umap) # Fit on UMAP-reduced data\n",
    "    wcss_umap.append(kmeans_umap.inertia_)\n",
    "    print(f\"✓ Calculated inertia for k={i} on UMAP data\")\n",
    "\n",
    "# Plot the Elbow Method graph for UMAP data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PLOTTING THE ELBOW METHOD GRAPH (UMAP DATA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.lineplot(x=list(k_range_umap), y=wcss_umap, marker='o', color='purple')\n",
    "plt.title('Elbow Method for Optimal k (UMAP-reduced Data)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)', fontsize=12)\n",
    "plt.xticks(k_range_umap)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhDE3NXTWpac"
   },
   "source": [
    "#  **Clustering on UMAP-Reduced Data (Various Methods):**\n",
    "    *   Applied K-Means, DBSCAN, Agglomerative, GMM, Affinity Propagation, Mean Shift, Spectral, Birch, and HDBSCAN clustering algorithms to the UMAP-reduced data.\n",
    "    *   Determined the optimal number of clusters for K-Means using the Elbow Method.\n",
    "    *   Tuned parameters for density-based methods (DBSCAN, HDBSCAN) using the K-distance plot.\n",
    "    *   Visualized the clusters found by each method in the 2D UMAP space.\n",
    "    *   Evaluated the performance of each clustering method using metrics like Silhouette, Calinski-Harabasz, and Davies-Bouldin scores.\n",
    "    *   Insights:\n",
    "        *   Different algorithms reveal different clustering structures in the data.\n",
    "        *   Evaluation metrics provide objective measures for comparing the quality of the clusters found by each method.\n",
    "        *   UMAP visualizations help in understanding the spatial separation and characteristics of the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "01e45cfb",
    "outputId": "fc65e65b-e96f-4bfc-a56d-9900e9097203"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  K-MEANS CLUSTERING ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"APPLYING K-MEANS CLUSTERING ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "if 'optimal_k' not in locals():\n",
    "    print(\"Warning: 'optimal_k' variable not found. Setting k to 5 as a default.\")\n",
    "    optimal_k = 5\n",
    "else:\n",
    "     print(f\"Using optimal_k = {optimal_k} for K-Means on UMAP data.\")\n",
    "\n",
    "\n",
    "# Initialize and fit the K-Means model on UMAP-reduced data\n",
    "kmeans_umap = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "kmeans_labels_umap = kmeans_umap.fit_predict(X_umap) # Fit and predict clusters on UMAP features\n",
    "\n",
    "# Add the K-Means cluster labels (from UMAP data) to the df_clean DataFrame\n",
    "df_clean['kmeans_cluster_umap'] = kmeans_labels_umap\n",
    "\n",
    "print(f\"✓ K-Means clustering completed on UMAP-reduced data with k = {optimal_k}.\")\n",
    "\n",
    "# Display the count of documents in each cluster\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"K-MEANS CLUSTER DISTRIBUTION (UMAP Data)\")\n",
    "print(\"=\"*80)\n",
    "cluster_distribution_umap = df_clean['kmeans_cluster_umap'].value_counts().sort_index()\n",
    "display(cluster_distribution_umap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72lPBb4kgCRG"
   },
   "source": [
    "# K-MEANS Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "id": "DwbqtcNzuwBZ",
    "outputId": "e8e6820a-0a0a-411e-e20c-4e828fbecaa6"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE K-MEANS CLUSTERS ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VISUALIZING K-MEANS CLUSTERS ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap (UMAP-reduced data) and kmeans_labels_umap (K-Means labels from UMAP data) are available\n",
    "if 'X_umap' in locals() and 'kmeans_labels_umap' in locals():\n",
    "    # Create a DataFrame for plotting\n",
    "    df_umap_clusters = pd.DataFrame(data=X_umap, columns=['UMAP1', 'UMAP2'])\n",
    "    df_umap_clusters['Cluster'] = kmeans_labels_umap\n",
    "\n",
    "    # Plot the clusters in the 2D UMAP space\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    sns.scatterplot(\n",
    "        x='UMAP1', y='UMAP2',\n",
    "        hue='Cluster',\n",
    "        palette=sns.color_palette(\"viridis\", n_colors=len(df_umap_clusters['Cluster'].unique())), # Use a color palette based on the number of unique clusters\n",
    "        data=df_umap_clusters,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.title('K-Means Clusters on UMAP-Reduced Data', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.legend(title='Cluster ID')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Error: Required variables (X_umap or kmeans_labels_umap) not found. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-1Fyf1VgMnu"
   },
   "source": [
    "# AGGLOMERATIVE Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "1ce78636",
    "outputId": "29d9ce0e-80ef-402d-dee9-b21591720fd5"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGLOMERATIVE CLUSTERING ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Apply Agglomerative Clustering to the UMAP-reduced features.\n",
    "\"\"\"\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"APPLYING AGGLOMERATIVE CLUSTERING ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap (UMAP-reduced data) is available\n",
    "if 'X_umap' not in locals():\n",
    "    print(\"Error: X_umap data not found. Please run the UMAP dimensionality reduction cell first.\")\n",
    "else:\n",
    "    n_clusters_agg = 5 # Using 5 for consistency with K-Means, can be adjusted\n",
    "    linkage = 'ward' # Common linkage for general purpose clustering\n",
    "\n",
    "    print(f\"Using Agglomerative Clustering on UMAP data with n_clusters={n_clusters_agg} and linkage='{linkage}'\")\n",
    "\n",
    "    agglomerative_umap = AgglomerativeClustering(n_clusters=n_clusters_agg, linkage=linkage)\n",
    "    agg_labels_umap = agglomerative_umap.fit_predict(X_umap) # Fit and predict clusters on UMAP features\n",
    "\n",
    "    # Add the Agglomerative cluster labels (from UMAP data) to the df_clean DataFrame\n",
    "    df_clean['agg_cluster_umap'] = agg_labels_umap\n",
    "\n",
    "    print(f\"✓ Agglomerative Clustering completed on UMAP-reduced data with n_clusters = {n_clusters_agg}.\")\n",
    "\n",
    "    # Display the count of documents in each cluster\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AGGLOMERATIVE CLUSTER DISTRIBUTION (UMAP Data)\")\n",
    "    print(\"=\"*80)\n",
    "    cluster_distribution_agg_umap = df_clean['agg_cluster_umap'].value_counts().sort_index()\n",
    "    display(cluster_distribution_agg_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "id": "5f5f9725",
    "outputId": "568e1863-fcec-4846-fb6b-be5c2ad7a996"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE AGGLOMERATIVE CLUSTERS ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Visualize the Agglomerative clusters in the 2D UMAP space.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VISUALIZING AGGLOMERATIVE CLUSTERS ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap (UMAP-reduced data) and agg_labels_umap (Agglomerative labels from UMAP data) are available\n",
    "if 'X_umap' in locals() and 'agg_labels_umap' in locals():\n",
    "    # Create a DataFrame for plotting\n",
    "    df_umap_agg = pd.DataFrame(data=X_umap, columns=['UMAP1', 'UMAP2'])\n",
    "    df_umap_agg['Cluster'] = agg_labels_umap # Use Agglomerative labels\n",
    "\n",
    "    # Plot the clusters in the 2D UMAP space\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    sns.scatterplot(\n",
    "        x='UMAP1', y='UMAP2',\n",
    "        hue='Cluster',\n",
    "        palette=sns.color_palette(\"viridis\", n_colors=len(df_umap_agg['Cluster'].unique())), # Use a color palette based on the number of unique clusters\n",
    "        data=df_umap_agg,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.title('Agglomerative Clusters on UMAP-Reduced Data', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.legend(title='Cluster ID')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: Required variables (X_umap or agg_labels_umap) not found. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuJIeqiSgSIx"
   },
   "source": [
    "# DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 882
    },
    "id": "v0KZ7RwHEsR9",
    "outputId": "4df14384-b30e-4ac0-c874-944eb08034b6"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: FIND OPTIMAL EPSILON (eps) FOR DBSCAN\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: CALCULATING K-DISTANCE PLOT TO FIND 'eps'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap is available\n",
    "if 'X_umap' not in locals():\n",
    "    print(\"Error: X_umap data not found. Please run the UMAP cell first.\")\n",
    "else:\n",
    "    # 'k' is usually set to your 'min_samples' value.\n",
    "    # We will use k=5 since your min_samples was 5.\n",
    "    k = 5\n",
    "    print(f\"Calculating distances to the {k}-th nearest neighbor...\")\n",
    "\n",
    "    # 1. Fit the NearestNeighbors model\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(X_umap)\n",
    "\n",
    "    # 2. Find the distances to the k-th neighbor for all points\n",
    "    distances, indices = nbrs.kneighbors(X_umap)\n",
    "\n",
    "    # 3. Get the k-th distance (the last column of 'distances') and sort it\n",
    "    k_distances = np.sort(distances[:, k-1], axis=0)\n",
    "\n",
    "    # 4. Create the plot\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(k_distances)\n",
    "    plt.title(f'K-Distance Plot (k={k})', fontsize=16)\n",
    "    plt.xlabel('Points (sorted by distance)', fontsize=12)\n",
    "    plt.ylabel(f'{k}-th Nearest Neighbor Distance (Your \"eps\" value)', fontsize=12)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" --> ACTION: Look at the plot.\")\n",
    "    print(\" --> Find the 'knee' (the elbow) - the point where the curve\")\n",
    "    print(\"     bends and starts to rise sharply.\")\n",
    "    print(\" --> The Y-axis value at this 'knee' is your new 'eps'.\")\n",
    "    print(\"     (It might be a value like 1.5, 2.8, etc.)\")\n",
    "    print(\"\\n --> Now, go to STEP 2 and update the 'eps' variable.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 984
    },
    "id": "hPrsABhTGxz8",
    "outputId": "661821d1-3aaa-48bd-e1a7-ebd3b5a7298a"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE DBSCAN PIPELINE (Cluster, Visualize, Evaluate)\n",
    "# ===========================================================================\n",
    "\n",
    "# --- 0. IMPORTS ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING COMPLETE DBSCAN PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap (UMAP-reduced data) is available\n",
    "if 'X_umap' not in locals():\n",
    "    print(\"Error: X_umap data not found. Please run the UMAP dimensionality reduction cell first.\")\n",
    "else:\n",
    "\n",
    "    # ============================================================================\n",
    "    # --- 1. APPLY DBSCAN CLUSTERING ---\n",
    "    # ============================================================================\n",
    "    print(\"\\n--- 1. APPLYING DBSCAN CLUSTERING ---\")\n",
    "\n",
    "    # Use the optimal parameters you found\n",
    "    eps = 0.6\n",
    "    min_samples = 5\n",
    "\n",
    "    print(f\"Using tuned parameters: eps={eps}, min_samples={min_samples}\")\n",
    "\n",
    "    dbscan_umap = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan_labels_umap = dbscan_umap.fit_predict(X_umap) # Fit and predict\n",
    "\n",
    "    # Add the DBSCAN cluster labels to the df_clean DataFrame (if it exists)\n",
    "    if 'df_clean' in locals():\n",
    "        df_clean['dbscan_cluster_umap'] = dbscan_labels_umap\n",
    "    else:\n",
    "        print(\"Warning: 'df_clean' DataFrame not found. Labels will not be saved to it.\")\n",
    "\n",
    "    n_clusters_dbscan_umap = len(set(dbscan_labels_umap)) - (1 if -1 in dbscan_labels_umap else 0)\n",
    "    n_noise_umap = list(dbscan_labels_umap).count(-1)\n",
    "\n",
    "    print(f\"✓ Clustering complete.\")\n",
    "    print(f\"  Estimated number of clusters: {n_clusters_dbscan_umap}\")\n",
    "    print(f\"  Estimated number of noise points: {n_noise_umap}\")\n",
    "\n",
    "\n",
    "    # ============================================================================\n",
    "    # --- 2. VISUALIZE DBSCAN CLUSTERS ---\n",
    "    # ============================================================================\n",
    "    print(\"\\n--- 2. VISUALIZING CLUSTERS ---\")\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    df_umap_dbscan = pd.DataFrame(data=X_umap, columns=['UMAP1', 'UMAP2'])\n",
    "    df_umap_dbscan['Cluster'] = dbscan_labels_umap # Use DBSCAN labels\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    # Create a color palette, ensuring noise points (-1) get a distinct color (e.g., gray)\n",
    "    unique_labels = sorted(df_umap_dbscan['Cluster'].unique())\n",
    "    colors = sns.color_palette(\"viridis\", n_colors=len(unique_labels) - (1 if -1 in unique_labels else 0))\n",
    "    if -1 in unique_labels:\n",
    "        colors.insert(unique_labels.index(-1), 'gray') # Assign gray to noise\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x='UMAP1', y='UMAP2',\n",
    "        hue='Cluster',\n",
    "        palette=colors, # Use the custom color palette\n",
    "        data=df_umap_dbscan,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.title(f'DBSCAN Clusters on UMAP (eps={eps})', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.legend(title='Cluster ID')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uOB6ID5ghKs"
   },
   "source": [
    "# HDBSCAN  Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 984
    },
    "id": "s1n5k3miHxD-",
    "outputId": "6a6ce5b8-29b7-4496-e593-de74384a422b"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE HDBSCAN PIPELINE (Cluster, Visualize, Evaluate)\n",
    "# ============================================================================\n",
    "try:\n",
    "    import hdbscan\n",
    "except ImportError:\n",
    "    print(\"=\"*80)\n",
    "    print(\"ERROR: 'hdbscan' library not found.\")\n",
    "    print(\"!pip install hdbscan\")\n",
    "    print(\"=\"*80)\n",
    "    # Stop execution if library is not found\n",
    "    raise\n",
    "\n",
    "# --- 0. IMPORTS ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING COMPLETE HDBSCAN PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap (UMAP-reduced data) is available\n",
    "if 'X_umap' not in locals():\n",
    "    print(\"Error: X_umap data not found. Please run the UMAP dimensionality reduction cell first.\")\n",
    "else:\n",
    "\n",
    "    # ============================================================================\n",
    "    # --- 1. APPLY HDBSCAN CLUSTERING ---\n",
    "    # ============================================================================\n",
    "    print(\"\\n--- 1. APPLYING HDBSCAN CLUSTERING ---\")\n",
    "\n",
    "    # The main parameter to tune is 'min_cluster_size'.\n",
    "    # We'll start with 5, which was a good 'min_samples' value for DBSCAN.\n",
    "    min_cluster_size = 5\n",
    "\n",
    "    print(f\"Using parameters: min_cluster_size={min_cluster_size}\")\n",
    "\n",
    "    # Initialize the HDBSCAN model\n",
    "    hdbscan_model = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        gen_min_span_tree=True # Good for visualization later if needed\n",
    "    )\n",
    "\n",
    "    # Fit and predict clusters\n",
    "    hdbscan_labels_umap = hdbscan_model.fit_predict(X_umap)\n",
    "\n",
    "    # Add the HDBSCAN cluster labels to the df_clean DataFrame (if it exists)\n",
    "    if 'df_clean' in locals():\n",
    "        df_clean['hdbscan_cluster_umap'] = hdbscan_labels_umap\n",
    "    else:\n",
    "        print(\"Warning: 'df_clean' DataFrame not found. Labels will not be saved to it.\")\n",
    "\n",
    "    # HDBSCAN also uses -1 for noise\n",
    "    n_clusters_hdbscan = len(set(hdbscan_labels_umap)) - (1 if -1 in hdbscan_labels_umap else 0)\n",
    "    n_noise_hdbscan = list(hdbscan_labels_umap).count(-1)\n",
    "\n",
    "    print(f\"✓ Clustering complete.\")\n",
    "    print(f\"  Estimated number of clusters: {n_clusters_hdbscan}\")\n",
    "    print(f\"  Estimated number of noise points: {n_noise_hdbscan}\")\n",
    "\n",
    "\n",
    "    # ============================================================================\n",
    "    # --- 2. VISUALIZE HDBSCAN CLUSTERS ---\n",
    "    # ============================================================================\n",
    "    print(\"\\n--- 2. VISUALIZING CLUSTERS ---\")\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    df_umap_hdbscan = pd.DataFrame(data=X_umap, columns=['UMAP1', 'UMAP2'])\n",
    "    df_umap_hdbscan['Cluster'] = hdbscan_labels_umap # Use HDBSCAN labels\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    # Create a color palette, ensuring noise points (-1) get a distinct color (e.g., gray)\n",
    "    unique_labels = sorted(df_umap_hdbscan['Cluster'].unique())\n",
    "    colors = sns.color_palette(\"deep\", n_colors=len(unique_labels) - (1 if -1 in unique_labels else 0))\n",
    "    if -1 in unique_labels:\n",
    "        colors.insert(unique_labels.index(-1), 'gray') # Assign gray to noise\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x='UMAP1', y='UMAP2',\n",
    "        hue='Cluster',\n",
    "        palette=colors,\n",
    "        data=df_umap_hdbscan,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.title(f'HDBSCAN Clusters on UMAP (min_cluster_size={min_cluster_size})', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.legend(title='Cluster ID')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-QVY0AggZ23"
   },
   "source": [
    "#AFFINITY PROPAGATION Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "nHkiFM8l1N5q",
    "outputId": "7345e368-f2c0-4d86-97a1-6f8387dbb867"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AFFINITY PROPAGATION CLUSTERING ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"APPLYING AFFINITY PROPAGATION CLUSTERING ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap (UMAP-reduced data) is available\n",
    "if 'X_umap' not in locals():\n",
    "    print(\"Error: X_umap data not found. Please run the UMAP dimensionality reduction cell first.\")\n",
    "else:\n",
    "    damping = 0.9 # Example value (default is 0.5), range [0.5, 1.0)\n",
    "    # preference = -50 # Example: Uncomment and set to control the number of clusters\n",
    "\n",
    "    print(f\"Using Affinity Propagation on UMAP data with damping={damping}\")\n",
    "\n",
    "    affinity_prop_umap = AffinityPropagation(damping=damping, random_state=42)\n",
    "\n",
    "    # Fit and predict clusters on UMAP features\n",
    "    affinity_labels_umap = affinity_prop_umap.fit_predict(X_umap)\n",
    "\n",
    "    # Add the Affinity Propagation cluster labels (from UMAP data) to the df_clean DataFrame\n",
    "    df_clean['affinity_cluster_umap'] = affinity_labels_umap\n",
    "\n",
    "    # Get the cluster centers (exemplars) found by the algorithm\n",
    "    cluster_centers_indices = affinity_prop_umap.cluster_centers_indices_\n",
    "    n_clusters_affinity_umap = len(cluster_centers_indices) if cluster_centers_indices is not None else 0\n",
    "\n",
    "    print(f\"✓ Affinity Propagation clustering completed on UMAP-reduced data.\")\n",
    "    print(f\"  Estimated number of clusters: {n_clusters_affinity_umap}\")\n",
    "\n",
    "    # Display the count of documents in each cluster\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AFFINITY PROPAGATION CLUSTER DISTRIBUTION (UMAP Data)\")\n",
    "    print(\"=\"*80)\n",
    "    cluster_distribution_affinity_umap = df_clean['affinity_cluster_umap'].value_counts().sort_index()\n",
    "    display(cluster_distribution_affinity_umap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "id": "fcLWjxKp1dn2",
    "outputId": "e62341a3-6a85-4bb1-db7a-8090161196a0"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE AFFINITY PROPAGATION CLUSTERS ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VISUALIZING AFFINITY PROPAGATION CLUSTERS ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap and affinity_labels_umap (Affinity Propagation labels from UMAP data) are available\n",
    "if 'X_umap' in locals() and 'affinity_labels_umap' in locals():\n",
    "    # Create a DataFrame for plotting\n",
    "    df_umap_affinity = pd.DataFrame(data=X_umap, columns=['UMAP1', 'UMAP2'])\n",
    "    df_umap_affinity['Cluster'] = affinity_labels_umap # Use Affinity Propagation labels\n",
    "\n",
    "    # Plot the clusters in the 2D UMAP space\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    # Create a color palette for the clusters.\n",
    "    # Affinity Propagation assigns all points to a cluster, so no noise (-1) handling is needed.\n",
    "    unique_labels = sorted(df_umap_affinity['Cluster'].unique())\n",
    "    n_clusters = len(unique_labels)\n",
    "\n",
    "    # Use a qualitative palette (like 'deep', 'tab10', or 'bright') for distinct clusters\n",
    "    colors = sns.color_palette(\"deep\", n_colors=n_clusters)\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x='UMAP1', y='UMAP2',\n",
    "        hue='Cluster',\n",
    "        palette=colors, # Use the custom color palette\n",
    "        data=df_umap_affinity,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.title('Affinity Propagation Clusters on UMAP-Reduced Data', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.legend(title='Cluster ID')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: Required variables (X_umap or affinity_labels_umap) not found. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO21KG05hDEO"
   },
   "source": [
    "# GAUSSIAN MIXTURE MODEL (GMM) Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "sKhzw-LV2zjY",
    "outputId": "29b4ac97-8c8e-4c51-f30d-8e082f3f3ef2"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GAUSSIAN MIXTURE MODEL (GMM) CLUSTERING ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Apply GMM (Gaussian Mixture Models) clustering to the UMAP-reduced features.\n",
    "\"\"\"\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"APPLYING GMM CLUSTERING ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap (UMAP-reduced data) is available\n",
    "if 'X_umap' not in locals():\n",
    "    print(\"Error: X_umap data not found. Please run the UMAP dimensionality reduction cell first.\")\n",
    "else:\n",
    "    n_components_gmm = 5\n",
    "\n",
    "    print(f\"Using GMM on UMAP data with n_components={n_components_gmm} (from Elbow Plot)\")\n",
    "\n",
    "    gmm_umap = GaussianMixture(n_components=n_components_gmm, random_state=42)\n",
    "\n",
    "    gmm_labels_umap = gmm_umap.fit_predict(X_umap)\n",
    "\n",
    "    # Add the GMM cluster labels (from UMAP data) to the df_clean DataFrame\n",
    "    df_clean['gmm_cluster_umap'] = gmm_labels_umap\n",
    "\n",
    "    # The number of clusters is what we defined in n_components\n",
    "    n_clusters_gmm = n_components_gmm\n",
    "\n",
    "    print(f\"✓ GMM clustering completed on UMAP-reduced data.\")\n",
    "    print(f\"  Number of clusters (n_components): {n_clusters_gmm}\")\n",
    "\n",
    "    # Display the count of documents in each cluster\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GMM CLUSTER DISTRIBUTION (UMAP Data)\")\n",
    "    print(\"=\"*80)\n",
    "    cluster_distribution_gmm_umap = df_clean['gmm_cluster_umap'].value_counts().sort_index()\n",
    "    display(cluster_distribution_gmm_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "id": "R0lzonRu21OK",
    "outputId": "b1217a07-9059-4618-fa1e-226417ec31fa"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE GMM CLUSTERS ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Visualize the GMM clusters in the 2D UMAP space.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VISUALIZING GMM CLUSTERS ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap and gmm_labels_umap (GMM labels from UMAP data) are available\n",
    "if 'X_umap' in locals() and 'gmm_labels_umap' in locals():\n",
    "    # Create a DataFrame for plotting\n",
    "    df_umap_gmm = pd.DataFrame(data=X_umap, columns=['UMAP1', 'UMAP2'])\n",
    "    df_umap_gmm['Cluster'] = gmm_labels_umap # Use GMM labels\n",
    "\n",
    "    # Plot the clusters in the 2D UMAP space\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    # Create a color palette for the clusters.\n",
    "    # GMM assigns all points to a cluster, so no noise (-1) handling is needed.\n",
    "    unique_labels = sorted(df_umap_gmm['Cluster'].unique())\n",
    "    n_clusters = len(unique_labels)\n",
    "\n",
    "    colors = sns.color_palette(\"deep\", n_colors=n_clusters)\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x='UMAP1', y='UMAP2',\n",
    "        hue='Cluster',\n",
    "        palette=colors, # Use the custom color palette\n",
    "        data=df_umap_gmm,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.title('GMM Clusters on UMAP-Reduced Data', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.legend(title='Cluster ID')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: Required variables (X_umap or gmm_labels_umap) not found. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-nJ7zyZhJ5e"
   },
   "source": [
    "# BIRCH Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "ac1932c6",
    "outputId": "72765f0b-7636-40b5-e27e-5a1a048d8ed7"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BIRCH CLUSTERING ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "from sklearn.cluster import Birch\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"APPLYING BIRCH CLUSTERING ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap (UMAP-reduced data) is available\n",
    "if 'X_umap' not in locals():\n",
    "    print(\"Error: X_umap data not found. Please run the UMAP dimensionality reduction cell first.\")\n",
    "else:\n",
    "    n_clusters_birch = None # Let Birch estimate the number of clusters, or set a value (e.g., 5)\n",
    "    threshold_birch = 0.5 # Example value, tune this\n",
    "    branching_factor_birch = 50 # Example value, tune this\n",
    "\n",
    "    if n_clusters_birch is not None:\n",
    "        print(f\"Using Birch on UMAP data with n_clusters={n_clusters_birch}, threshold={threshold_birch}, branching_factor={branching_factor_birch}\")\n",
    "        birch_umap = Birch(n_clusters=n_clusters_birch, threshold=threshold_birch, branching_factor=branching_factor_birch)\n",
    "    else:\n",
    "        print(f\"Using Birch on UMAP data, automatically determining n_clusters with threshold={threshold_birch}, branching_factor={branching_factor_birch}\")\n",
    "        birch_umap = Birch(threshold=threshold_birch, branching_factor=branching_factor_birch)\n",
    "\n",
    "\n",
    "    # Fit and predict clusters on UMAP features\n",
    "    birch_labels_umap = birch_umap.fit_predict(X_umap)\n",
    "\n",
    "    # Add the Birch cluster labels (from UMAP data) to the df_clean DataFrame\n",
    "    df_clean['birch_cluster_umap'] = birch_labels_umap\n",
    "\n",
    "    # Get the number of clusters found by Birch\n",
    "    n_clusters_birch_found = len(set(birch_labels_umap))\n",
    "\n",
    "    print(f\"✓ Birch clustering completed on UMAP-reduced data.\")\n",
    "    print(f\"  Estimated number of clusters found by Birch: {n_clusters_birch_found}\")\n",
    "\n",
    "\n",
    "    # Display the count of documents in each cluster\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BIRCH CLUSTER DISTRIBUTION (UMAP Data)\")\n",
    "    print(\"=\"*80)\n",
    "    cluster_distribution_birch_umap = df_clean['birch_cluster_umap'].value_counts().sort_index()\n",
    "    display(cluster_distribution_birch_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "id": "b5d3fa01",
    "outputId": "034eed47-6a42-4f9d-89f2-6a497fb42231"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE BIRCH CLUSTERS ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Visualize the Birch clusters in the 2D UMAP space.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VISUALIZING BIRCH CLUSTERS ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap and birch_labels_umap (Birch labels from UMAP data) are available\n",
    "if 'X_umap' in locals() and 'birch_labels_umap' in locals():\n",
    "    # Create a DataFrame for plotting\n",
    "    df_umap_birch = pd.DataFrame(data=X_umap, columns=['UMAP1', 'UMAP2'])\n",
    "    df_umap_birch['Cluster'] = birch_labels_umap # Use Birch labels\n",
    "\n",
    "    # Plot the clusters in the 2D UMAP space\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    # Create a color palette for the clusters.\n",
    "    unique_labels = sorted(df_umap_birch['Cluster'].unique())\n",
    "    n_clusters = len(unique_labels)\n",
    "\n",
    "    colors = sns.color_palette(\"tab20\", n_colors=n_clusters) # Use a palette suitable for potentially more clusters\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x='UMAP1', y='UMAP2',\n",
    "        hue='Cluster',\n",
    "        palette=colors, # Use the custom color palette\n",
    "        data=df_umap_birch,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.title('Birch Clusters on UMAP-Reduced Data', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.legend(title='Cluster ID')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Error: Required variables (X_umap or birch_labels_umap) not found. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfbpoavdhSeP"
   },
   "source": [
    "# OPTICS Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "4b8ff808",
    "outputId": "378ab004-708c-47f9-c566-6b32981576ff"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTICS CLUSTERING ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Apply OPTICS (Ordering Points To Identify the Clustering Structure) clustering\n",
    "to the UMAP-reduced features.\n",
    "\"\"\"\n",
    "from sklearn.cluster import OPTICS\n",
    "import pandas as pd\n",
    "import numpy as np # Import numpy for inf\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"APPLYING OPTICS CLUSTERING ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap (UMAP-reduced data) is available\n",
    "if 'X_umap' not in locals():\n",
    "    print(\"Error: X_umap data not found. Please run the UMAP dimensionality reduction cell first.\")\n",
    "else:\n",
    "    min_samples_optics = 5 # Example value, tune this\n",
    "    max_eps_optics = np.inf # Example value, tune this\n",
    "\n",
    "    print(f\"Using OPTICS on UMAP data with min_samples={min_samples_optics} and max_eps={max_eps_optics}\")\n",
    "\n",
    "    # Note: OPTICS can be computationally intensive for large datasets.\n",
    "    optics_umap = OPTICS(min_samples=min_samples_optics, max_eps=max_eps_optics)\n",
    "\n",
    "    # Fit the model\n",
    "    optics_umap.fit(X_umap)\n",
    "\n",
    "    optics_labels_umap = optics_umap.labels_\n",
    "\n",
    "    # Add the OPTICS cluster labels (from UMAP data) to the df_clean DataFrame\n",
    "    df_clean['optics_cluster_umap'] = optics_labels_umap\n",
    "\n",
    "    # OPTICS labels -1 indicate noise points (outliers)\n",
    "    n_clusters_optics_umap = len(set(optics_labels_umap)) - (1 if -1 in optics_labels_umap else 0)\n",
    "    n_noise_umap_optics = list(optics_labels_umap).count(-1)\n",
    "\n",
    "    print(f\"✓ OPTICS clustering completed on UMAP-reduced data.\")\n",
    "    print(f\" Estimated number of clusters: {n_clusters_optics_umap}\")\n",
    "    print(f\" Estimated number of noise points: {n_noise_umap_optics}\")\n",
    "\n",
    "\n",
    "    # Display the count of documents in each cluster (including noise)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OPTICS CLUSTER DISTRIBUTION (UMAP Data - INCLUDING NOISE)\")\n",
    "    print(\"=\"*80)\n",
    "    cluster_distribution_optics_umap = df_clean['optics_cluster_umap'].value_counts().sort_index()\n",
    "    display(cluster_distribution_optics_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "id": "8af82e75",
    "outputId": "1def608b-ceb5-4ffa-da10-0ad2a5b1604c"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE OPTICS CLUSTERS ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Visualize the OPTICS clusters in the 2D UMAP space, including noise points.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VISUALIZING OPTICS CLUSTERS ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap and optics_labels_umap (OPTICS labels from UMAP data) are available\n",
    "if 'X_umap' in locals() and 'optics_labels_umap' in locals():\n",
    "    # Create a DataFrame for plotting\n",
    "    df_umap_optics = pd.DataFrame(data=X_umap, columns=['UMAP1', 'UMAP2'])\n",
    "    df_umap_optics['Cluster'] = optics_labels_umap # Use OPTICS labels\n",
    "\n",
    "    # Plot the clusters in the 2D UMAP space\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    # Create a color palette, ensuring noise points (-1) get a distinct color (e.g., black or gray)\n",
    "    unique_labels = sorted(df_umap_optics['Cluster'].unique())\n",
    "    colors = sns.color_palette(\"viridis\", n_colors=len(unique_labels) - (1 if -1 in unique_labels else 0))\n",
    "    if -1 in unique_labels:\n",
    "        colors.insert(unique_labels.index(-1), 'gray') # Assign gray to noise\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x='UMAP1', y='UMAP2',\n",
    "        hue='Cluster',\n",
    "        palette=colors, # Use the custom color palette\n",
    "        data=df_umap_optics,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.title('OPTICS Clusters on UMAP-Reduced Data', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.legend(title='Cluster ID')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Error: Required variables (X_umap or optics_labels_umap) not found. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fwMHgEvhaxJ"
   },
   "source": [
    "# SPECTRAL Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "3e39e2b8",
    "outputId": "e1b46028-df7c-46ed-d8f0-52252c592431"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SPECTRAL CLUSTERING ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Apply Spectral Clustering to the UMAP-reduced features.\n",
    "\"\"\"\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"APPLYING SPECTRAL CLUSTERING ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap (UMAP-reduced data) is available\n",
    "if 'X_umap' not in locals():\n",
    "    print(\"Error: X_umap data not found. Please run the UMAP dimensionality reduction cell first.\")\n",
    "else:\n",
    "\n",
    "    n_clusters_spectral = 5 # Using 5 for consistency with K-Means, can be adjusted\n",
    "\n",
    "    print(f\"Using Spectral Clustering on UMAP data with n_clusters={n_clusters_spectral}\")\n",
    "\n",
    "    # Using 'kmeans' for assign_labels is common after spectral embedding\n",
    "    spectral_umap = SpectralClustering(n_clusters=n_clusters_spectral, assign_labels='kmeans', random_state=42)\n",
    "\n",
    "\n",
    "    spectral_labels_umap = spectral_umap.fit_predict(X_umap)\n",
    "\n",
    "    # Add the Spectral cluster labels (from UMAP data) to the df_clean DataFrame\n",
    "    df_clean['spectral_cluster_umap'] = spectral_labels_umap\n",
    "\n",
    "    print(f\"✓ Spectral Clustering completed on UMAP-reduced data with n_clusters = {n_clusters_spectral}.\")\n",
    "\n",
    "    # Display the count of documents in each cluster\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SPECTRAL CLUSTER DISTRIBUTION (UMAP Data)\")\n",
    "    print(\"=\"*80)\n",
    "    cluster_distribution_spectral_umap = df_clean['spectral_cluster_umap'].value_counts().sort_index()\n",
    "    display(cluster_distribution_spectral_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "id": "4ff9c0aa",
    "outputId": "31267ce0-a4ae-4a53-aba4-9b9b0998741f"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE SPECTRAL CLUSTERS ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Visualize the Spectral clusters in the 2D UMAP space.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VISUALIZING SPECTRAL CLUSTERS ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap and spectral_labels_umap (Spectral labels from UMAP data) are available\n",
    "if 'X_umap' in locals() and 'spectral_labels_umap' in locals():\n",
    "    # Create a DataFrame for plotting\n",
    "    df_umap_spectral = pd.DataFrame(data=X_umap, columns=['UMAP1', 'UMAP2'])\n",
    "    df_umap_spectral['Cluster'] = spectral_labels_umap # Use Spectral labels\n",
    "\n",
    "    # Plot the clusters in the 2D UMAP space\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    # Create a color palette for the clusters.\n",
    "    unique_labels = sorted(df_umap_spectral['Cluster'].unique())\n",
    "    n_clusters = len(unique_labels)\n",
    "\n",
    "    colors = sns.color_palette(\"tab20\", n_colors=n_clusters) # Use a palette suitable for more clusters\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x='UMAP1', y='UMAP2',\n",
    "        hue='Cluster',\n",
    "        palette=colors, # Use the custom color palette\n",
    "        data=df_umap_spectral,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.title('Spectral Clusters on UMAP-Reduced Data', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.legend(title='Cluster ID')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Error: Required variables (X_umap or spectral_labels_umap) not found. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nibmHqYWhgge"
   },
   "source": [
    "# MEAN SHIFT  Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "c1596ec5",
    "outputId": "a60575e9-c81b-4ab5-e7b4-bab98ba4b62b"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEAN SHIFT CLUSTERING ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"APPLYING MEAN SHIFT CLUSTERING ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap (UMAP-reduced data) is available\n",
    "if 'X_umap' not in locals():\n",
    "    print(\"Error: X_umap data not found. Please run the UMAP dimensionality reduction cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        bandwidth = estimate_bandwidth(X_umap, quantile=0.2, n_samples=500) # Adjust quantile as needed\n",
    "        print(f\"Estimated bandwidth: {bandwidth:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not estimate bandwidth: {e}. Setting a default value.\")\n",
    "        bandwidth = 1.0 # Set a default if estimation fails or data is too small\n",
    "\n",
    "\n",
    "\n",
    "    if bandwidth is not None:\n",
    "        mean_shift_umap = MeanShift(bandwidth=bandwidth, cluster_all=True) # cluster_all=True to avoid noise labels\n",
    "        print(f\"Using Mean Shift on UMAP data with bandwidth={bandwidth:.4f}\")\n",
    "    else:\n",
    "        mean_shift_umap = MeanShift(cluster_all=True) # Let the algorithm estimate bandwidth\n",
    "        print(\"Using Mean Shift on UMAP data, bandwidth will be estimated.\")\n",
    "\n",
    "\n",
    "    # Fit and predict clusters on UMAP features\n",
    "    mean_shift_labels_umap = mean_shift_umap.fit_predict(X_umap)\n",
    "\n",
    "    # Add the Mean Shift cluster labels (from UMAP data) to the df_clean DataFrame\n",
    "    df_clean['meanshift_cluster_umap'] = mean_shift_labels_umap\n",
    "\n",
    "    # Get the number of clusters found\n",
    "    cluster_centers = mean_shift_umap.cluster_centers_\n",
    "    n_clusters_meanshift_umap = len(cluster_centers)\n",
    "\n",
    "    print(f\"✓ Mean Shift clustering completed on UMAP-reduced data.\")\n",
    "    print(f\"  Estimated number of clusters: {n_clusters_meanshift_umap}\")\n",
    "\n",
    "    # Display the count of documents in each cluster\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MEAN SHIFT CLUSTER DISTRIBUTION (UMAP Data)\")\n",
    "    print(\"=\"*80)\n",
    "    cluster_distribution_meanshift_umap = df_clean['meanshift_cluster_umap'].value_counts().sort_index()\n",
    "    display(cluster_distribution_meanshift_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "id": "727e3c6f",
    "outputId": "b7890856-1e3f-4801-e2fc-9d3c0a379d0c"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE MEAN SHIFT CLUSTERS ON UMAP-REDUCED DATA\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VISUALIZING MEAN SHIFT CLUSTERS ON UMAP-REDUCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure X_umap and mean_shift_labels_umap (Mean Shift labels from UMAP data) are available\n",
    "if 'X_umap' in locals() and 'mean_shift_labels_umap' in locals():\n",
    "    # Create a DataFrame for plotting\n",
    "    df_umap_meanshift = pd.DataFrame(data=X_umap, columns=['UMAP1', 'UMAP2'])\n",
    "    df_umap_meanshift['Cluster'] = mean_shift_labels_umap # Use Mean Shift labels\n",
    "\n",
    "    # Plot the clusters in the 2D UMAP space\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    # Create a color palette for the clusters.\n",
    "    unique_labels = sorted(df_umap_meanshift['Cluster'].unique())\n",
    "    n_clusters = len(unique_labels)\n",
    "\n",
    "    # Use a qualitative palette for distinct clusters\n",
    "    colors = sns.color_palette(\"tab10\", n_colors=n_clusters)\n",
    "\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x='UMAP1', y='UMAP2',\n",
    "        hue='Cluster',\n",
    "        palette=colors, # Use the custom color palette\n",
    "        data=df_umap_meanshift,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.title('Mean Shift Clusters on UMAP-Reduced Data', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.legend(title='Cluster ID')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: Required variables (X_umap or mean_shift_labels_umap) not found. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMEZ1QTyhoy6"
   },
   "source": [
    "# Model Performance Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "4ece5e6e",
    "outputId": "0b2e5508-326c-451c-a5ac-1b6c5b494c9a"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL COMBINED CLUSTERING PERFORMANCE EVALUATION (UMAP Data)\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL COMBINED CLUSTERING PERFORMANCE EVALUATION (UMAP Data)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure UMAP data and all cluster labels are available\n",
    "required_labels = ['kmeans_cluster_umap', 'dbscan_cluster_umap', 'agg_cluster_umap',\n",
    "                   'gmm_cluster_umap', 'affinity_cluster_umap', 'meanshift_cluster_umap',\n",
    "                   'spectral_cluster_umap', 'birch_cluster_umap',\n",
    "                   'hdbscan_cluster_umap'  # <-- ADDED\n",
    "                  ]\n",
    "\n",
    "# Check if df_clean exists and has all required columns\n",
    "if 'df_clean' not in locals():\n",
    "     print(\"Error: 'df_clean' DataFrame not found. Please run all clustering cells.\")\n",
    "elif 'X_umap' not in locals():\n",
    "    print(\"Error: 'X_umap' data not found. Please run the UMAP cell.\")\n",
    "elif any(label not in df_clean.columns for label in required_labels):\n",
    "    print(\"Error: Not all required cluster label columns were found in df_clean.\")\n",
    "    print(\"Missing columns:\")\n",
    "    for label in required_labels:\n",
    "        if label not in df_clean.columns:\n",
    "            print(f\"  - {label}\")\n",
    "else:\n",
    "    performance_metrics_final = {}\n",
    "\n",
    "    # --- K-Means (UMAP) ---\n",
    "    try:\n",
    "        score_kmeans_silhouette = silhouette_score(X_umap, df_clean['kmeans_cluster_umap'])\n",
    "        score_kmeans_calinski = calinski_harabasz_score(X_umap, df_clean['kmeans_cluster_umap'])\n",
    "        score_kmeans_davies = davies_bouldin_score(X_umap, df_clean['kmeans_cluster_umap'])\n",
    "        performance_metrics_final['K-Means (UMAP)'] = {\n",
    "            'Silhouette Score': score_kmeans_silhouette,\n",
    "            'Calinski-Harabasz Score': score_kmeans_calinski,\n",
    "            'Davies-Bouldin Score': score_kmeans_davies\n",
    "        }\n",
    "        print(\"✓ Calculated metrics for K-Means on UMAP data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate metrics for K-Means on UMAP data: {e}\")\n",
    "        performance_metrics_final['K-Means (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "\n",
    "    # --- DBSCAN (UMAP) ---\n",
    "    # Evaluate on non-noise points if possible clusters > 1\n",
    "    dbscan_labels_umap = df_clean['dbscan_cluster_umap']\n",
    "    non_noise_mask_umap = dbscan_labels_umap != -1\n",
    "    X_umap_non_noise = X_umap[non_noise_mask_umap]\n",
    "    dbscan_labels_umap_non_noise = dbscan_labels_umap[non_noise_mask_umap]\n",
    "\n",
    "    if len(set(dbscan_labels_umap_non_noise)) > 1:\n",
    "        try:\n",
    "            score_dbscan_silhouette = silhouette_score(X_umap_non_noise, dbscan_labels_umap_non_noise)\n",
    "            score_dbscan_calinski = calinski_harabasz_score(X_umap_non_noise, dbscan_labels_umap_non_noise)\n",
    "            score_dbscan_davies = davies_bouldin_score(X_umap_non_noise, dbscan_labels_umap_non_noise)\n",
    "            performance_metrics_final['DBSCAN (UMAP)'] = {\n",
    "                'Silhouette Score': score_dbscan_silhouette,\n",
    "                'Calinski-Harabasz Score': score_dbscan_calinski,\n",
    "                'Davies-Bouldin Score': score_dbscan_davies\n",
    "            }\n",
    "            print(\"✓ Calculated metrics for DBSCAN on UMAP data (non-noise points).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate metrics for DBSCAN on UMAP data: {e}\")\n",
    "            performance_metrics_final['DBSCAN (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "    else:\n",
    "        print(\"DBSCAN on UMAP resulted in 1 or fewer non-noise clusters. Skipping metric calculation.\")\n",
    "        performance_metrics_final['DBSCAN (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "\n",
    "    # --- Agglomerative Clustering (UMAP) ---\n",
    "    try:\n",
    "        score_agg_silhouette = silhouette_score(X_umap, df_clean['agg_cluster_umap'])\n",
    "        score_agg_calinski = calinski_harabasz_score(X_umap, df_clean['agg_cluster_umap'])\n",
    "        score_agg_davies = davies_bouldin_score(X_umap, df_clean['agg_cluster_umap'])\n",
    "        performance_metrics_final['Agglomerative (UMAP)'] = {\n",
    "            'Silhouette Score': score_agg_silhouette,\n",
    "            'Calinski-Harabasz Score': score_agg_calinski,\n",
    "            'Davies-Bouldin Score': score_agg_davies\n",
    "        }\n",
    "        print(\"✓ Calculated metrics for Agglomerative Clustering on UMAP data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate metrics for Agglomerative Clustering on UMAP data: {e}\")\n",
    "        performance_metrics_final['Agglomerative (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "\n",
    "    # --- GMM (UMAP) ---\n",
    "    try:\n",
    "        score_gmm_silhouette = silhouette_score(X_umap, df_clean['gmm_cluster_umap'])\n",
    "        score_gmm_calinski = calinski_harabasz_score(X_umap, df_clean['gmm_cluster_umap'])\n",
    "        score_gmm_davies = davies_bouldin_score(X_umap, df_clean['gmm_cluster_umap'])\n",
    "        performance_metrics_final['GMM (UMAP)'] = {\n",
    "            'Silhouette Score': score_gmm_silhouette,\n",
    "            'Calinski-Harabasz Score': score_gmm_calinski,\n",
    "            'Davies-Bouldin Score': score_gmm_davies\n",
    "        }\n",
    "        print(\"✓ Calculated metrics for GMM on UMAP data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate metrics for GMM on UMAP data: {e}\")\n",
    "        performance_metrics_final['GMM (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "\n",
    "    # --- Affinity Propagation (UMAP) ---\n",
    "    affinity_labels_umap = df_clean['affinity_cluster_umap']\n",
    "    if len(set(affinity_labels_umap)) > 1:\n",
    "        try:\n",
    "            score_affinity_silhouette = silhouette_score(X_umap, affinity_labels_umap)\n",
    "            score_affinity_calinski = calinski_harabasz_score(X_umap, affinity_labels_umap)\n",
    "            score_affinity_davies = davies_bouldin_score(X_umap, affinity_labels_umap)\n",
    "            performance_metrics_final['Affinity Propagation (UMAP)'] = {\n",
    "                'Silhouette Score': score_affinity_silhouette,\n",
    "                'Calinski-Harabasz Score': score_affinity_calinski,\n",
    "                'Davies-Bouldin Score': score_affinity_davies\n",
    "            }\n",
    "            print(\"✓ Calculated metrics for Affinity Propagation on UMAP data.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate metrics for Affinity Propagation on UMAP data: {e}\")\n",
    "            performance_metrics_final['Affinity Propagation (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "    else:\n",
    "        print(\"Affinity Propagation on UMAP resulted in 1 cluster. Skipping metric calculation.\")\n",
    "        performance_metrics_final['Affinity Propagation (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "\n",
    "\n",
    "    # --- Mean Shift (UMAP) ---\n",
    "    meanshift_labels_umap = df_clean['meanshift_cluster_umap']\n",
    "    if len(set(meanshift_labels_umap)) > 1:\n",
    "        try:\n",
    "            score_meanshift_silhouette = silhouette_score(X_umap, meanshift_labels_umap)\n",
    "            score_meanshift_calinski = calinski_harabasz_score(X_umap, meanshift_labels_umap)\n",
    "            score_meanshift_davies = davies_bouldin_score(X_umap, meanshift_labels_umap)\n",
    "            performance_metrics_final['Mean Shift (UMAP)'] = {\n",
    "                'Silhouette Score': score_meanshift_silhouette,\n",
    "                'Calinski-Harabasz Score': score_meanshift_calinski,\n",
    "                'Davies-Bouldin Score': score_meanshift_davies\n",
    "            }\n",
    "            print(\"✓ Calculated metrics for Mean Shift on UMAP data.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate metrics for Mean Shift on UMAP data: {e}\")\n",
    "            performance_metrics_final['Mean Shift (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "    else:\n",
    "        print(\"Mean Shift on UMAP resulted in 1 cluster. Skipping metric calculation.\")\n",
    "        performance_metrics_final['Mean Shift (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "\n",
    "    # --- Spectral Clustering (UMAP) ---\n",
    "    try:\n",
    "        score_spectral_silhouette = silhouette_score(X_umap, df_clean['spectral_cluster_umap'])\n",
    "        score_spectral_calinski = calinski_harabasz_score(X_umap, df_clean['spectral_cluster_umap'])\n",
    "        score_spectral_davies = davies_bouldin_score(X_umap, df_clean['spectral_cluster_umap'])\n",
    "        performance_metrics_final['Spectral Clustering (UMAP)'] = {\n",
    "            'Silhouette Score': score_spectral_silhouette,\n",
    "            'Calinski-Harabasz Score': score_spectral_calinski,\n",
    "            'Davies-Bouldin Score': score_spectral_davies\n",
    "        }\n",
    "        print(\"✓ Calculated metrics for Spectral Clustering on UMAP data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate metrics for Spectral Clustering on UMAP data: {e}\")\n",
    "        performance_metrics_final['Spectral Clustering (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "\n",
    "    # --- Birch (UMAP) ---\n",
    "    birch_labels_umap = df_clean['birch_cluster_umap']\n",
    "    if len(set(birch_labels_umap)) > 1:\n",
    "        try:\n",
    "            score_birch_silhouette = silhouette_score(X_umap, birch_labels_umap)\n",
    "            score_birch_calinski = calinski_harabasz_score(X_umap, birch_labels_umap)\n",
    "            score_birch_davies = davies_bouldin_score(X_umap, birch_labels_umap)\n",
    "            performance_metrics_final['Birch (UMAP)'] = {\n",
    "                'Silhouette Score': score_birch_silhouette,\n",
    "                'Calinski-Harabasz Score': score_birch_calinski,\n",
    "                'Davies-Bouldin Score': score_birch_davies\n",
    "            }\n",
    "            print(\"✓ Calculated metrics for Birch on UMAP data.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate metrics for Birch on UMAP data: {e}\")\n",
    "            performance_metrics_final['Birch (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "    else:\n",
    "        print(\"Birch on UMAP resulted in 1 cluster. Skipping metric calculation.\")\n",
    "        performance_metrics_final['Birch (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "\n",
    "\n",
    "    # --- HDBSCAN (UMAP) ---  <-- NEW BLOCK\n",
    "    # Evaluate on non-noise points if possible clusters > 1\n",
    "    hdbscan_labels_umap = df_clean['hdbscan_cluster_umap']\n",
    "    non_noise_mask_hdbscan = hdbscan_labels_umap != -1\n",
    "    X_umap_non_noise_hdbscan = X_umap[non_noise_mask_hdbscan]\n",
    "    hdbscan_labels_umap_non_noise = hdbscan_labels_umap[non_noise_mask_hdbscan]\n",
    "\n",
    "    if len(set(hdbscan_labels_umap_non_noise)) > 1:\n",
    "        try:\n",
    "            score_hdbscan_silhouette = silhouette_score(X_umap_non_noise_hdbscan, hdbscan_labels_umap_non_noise)\n",
    "            score_hdbscan_calinski = calinski_harabasz_score(X_umap_non_noise_hdbscan, hdbscan_labels_umap_non_noise)\n",
    "            score_hdbscan_davies = davies_bouldin_score(X_umap_non_noise_hdbscan, hdbscan_labels_umap_non_noise)\n",
    "            performance_metrics_final['HDBSCAN (UMAP)'] = {\n",
    "                'Silhouette Score': score_hdbscan_silhouette,\n",
    "                'Calinski-Harabasz Score': score_hdbscan_calinski,\n",
    "                'Davies-Bouldin Score': score_hdbscan_davies\n",
    "            }\n",
    "            print(\"✓ Calculated metrics for HDBSCAN on UMAP data (non-noise points).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate metrics for HDBSCAN on UMAP data: {e}\")\n",
    "            performance_metrics_final['HDBSCAN (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "    else:\n",
    "        print(\"HDBSCAN on UMAP resulted in 1 or fewer non-noise clusters. Skipping metric calculation.\")\n",
    "        performance_metrics_final['HDBSCAN (UMAP)'] = {'Silhouette Score': None, 'Calinski-Harabasz Score': None, 'Davies-Bouldin Score': None}\n",
    "\n",
    "\n",
    "    # Display the performance metrics in a DataFrame\n",
    "    df_performance_metrics_final_umap = pd.DataFrame.from_dict(performance_metrics_final, orient='index')\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL COMBINED METRICS TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    display(df_performance_metrics_final_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d14e48aa",
    "outputId": "b2dbfac3-fbe5-4dd6-bedf-3b288e7452c6"
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt\n",
    "print(\"Generated requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l3frkuDUBucr",
    "outputId": "4caee43a-d25c-460b-d6f9-aa22b74e058e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the raw metric data\n",
    "data = {\n",
    "    \"Model\": [\n",
    "        \"K-Means (UMAP)\", \"DBSCAN (UMAP)\", \"Agglomerative (UMAP)\",\n",
    "        \"GMM (UMAP)\", \"Affinity Propagation (UMAP)\", \"Mean Shift (UMAP)\",\n",
    "        \"Spectral Clustering (UMAP)\", \"Birch (UMAP)\",\n",
    "        \"HDBSCAN (UMAP)\"  # <-- ADDED\n",
    "    ],\n",
    "    \"Silhouette Score\": [\n",
    "        0.751649, 0.885347, 0.751649, 0.751649, 0.808822,\n",
    "        0.861626, 0.172477, 0.725775,\n",
    "        0.793199  # <-- ADDED\n",
    "    ],\n",
    "    \"Calinski-Harabasz Score\": [\n",
    "        548.744873, 5956.537109, 548.744873, 548.744873,\n",
    "        1309.111328, 417.425995, 26.505274, 248.667938,\n",
    "        545.636963  # <-- ADDED\n",
    "    ],\n",
    "    \"Davies-Bouldin Score\": [\n",
    "        0.388494, 0.117261, 0.388494, 0.388494, 0.164639,\n",
    "        0.158811, 4.748195, 0.417121,\n",
    "        0.306198  # <-- ADDED\n",
    "    ]\n",
    "}\n",
    "\n",
    "evaluation_df = pd.DataFrame(data)\n",
    "\n",
    "# Sort the data for ranking\n",
    "df_s = evaluation_df.sort_values(by='Silhouette Score', ascending=False)\n",
    "df_c = evaluation_df.sort_values(by='Calinski-Harabasz Score', ascending=False)\n",
    "df_d = evaluation_df.sort_values(by='Davies-Bouldin Score', ascending=True) # Ascending is better\n",
    "\n",
    "# --- Create the plots ---\n",
    "plt.style.use('default')\n",
    "\n",
    "# Create three separate subplots, arranged vertically (3 rows, 1 column)\n",
    "# Make the figure taller to accommodate them\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 22))\n",
    "fig.suptitle(\"Clustering Metric Comparison (Raw Scores)\", fontsize=18, y=1.02)\n",
    "\n",
    "# --- Plot 1: Silhouette Score (Higher is Better) ---\n",
    "sns.barplot(\n",
    "    y=df_s['Silhouette Score'],\n",
    "    x=df_s['Model'],\n",
    "    ax=axes[0],\n",
    "    color='C0' # Standard blue\n",
    ")\n",
    "axes[0].set_title(\"Silhouette Score (Higher is Better)\", fontsize=16)\n",
    "axes[0].set_ylabel(\"Score\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Model\", fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45) # Rotate x-axis labels\n",
    "\n",
    "# --- Plot 2: Calinski-Harabasz Score (Higher is Better) ---\n",
    "sns.barplot(\n",
    "    y=df_c['Calinski-Harabasz Score'],\n",
    "    x=df_c['Model'],\n",
    "    ax=axes[1],\n",
    "    color='C1' # Standard orange\n",
    ")\n",
    "axes[1].set_title(\"Calinski-Harabasz Score (Higher is Better)\", fontsize=16)\n",
    "axes[1].set_ylabel(\"Score (Log Scale)\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Model\", fontsize=12)\n",
    "# A log scale is required on the y-axis\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].tick_params(axis='x', rotation=45) # Rotate x-axis labels\n",
    "\n",
    "# --- Plot 3: Davies-Bouldin Score (Lower is Better) ---\n",
    "sns.barplot(\n",
    "    y=df_d['Davies-Bouldin Score'],\n",
    "    x=df_d['Model'],\n",
    "    ax=axes[2],\n",
    "    color='C2' # Standard green\n",
    ")\n",
    "axes[2].set_title(\"Davies-Bouldin Score (Lower is Better)\", fontsize=16)\n",
    "axes[2].set_ylabel(\"Score\", fontsize=12)\n",
    "axes[2].set_xlabel(\"Model\", fontsize=12)\n",
    "axes[2].tick_params(axis='x', rotation=45) # Rotate x-axis labels\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lNGE114W2lc"
   },
   "source": [
    "\n",
    "#   **Analyze Sentiment Distribution within Clusters (All Methods on UMAP Data):**\n",
    "    *   Examined the distribution of original sentiment, ML predicted sentiment, and VADER sentiment within the clusters found by each method.\n",
    "    *   Insights:\n",
    "        *   This analysis helps understand the sentiment composition of each cluster.\n",
    "        *   Clusters with a high concentration of a specific sentiment can reveal underlying themes or topics associated with that sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2ba732b5",
    "outputId": "030899b1-0486-4e71-86b5-b98b1face48d"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYZE SENTIMENT DISTRIBUTION WITHIN CLUSTERS (ALL METHODS ON UMAP DATA)\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SENTIMENT DISTRIBUTION ANALYSIS WITHIN CLUSTERS (UMAP Data)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure df_clean exists and has all required cluster label columns\n",
    "required_labels = ['kmeans_cluster_umap', 'dbscan_cluster_umap', 'agg_cluster_umap',\n",
    "                   'gmm_cluster_umap', 'affinity_cluster_umap', 'meanshift_cluster_umap',\n",
    "                   'spectral_cluster_umap', 'birch_cluster_umap', 'hdbscan_cluster_umap']\n",
    "\n",
    "if 'df_clean' not in locals():\n",
    "     print(\"Error: 'df_clean' DataFrame not found. Please run all clustering cells.\")\n",
    "elif any(label not in df_clean.columns for label in required_labels):\n",
    "    print(\"Error: Not all required cluster label columns were found in df_clean.\")\n",
    "    print(\"Missing columns:\")\n",
    "    for label in required_labels:\n",
    "        if label not in df_clean.columns:\n",
    "            print(f\"  - {label}\")\n",
    "else:\n",
    "    # Define the sentiment columns to analyze\n",
    "    sentiment_cols = ['Sentiment', 'predicted_sentiment_ml', 'vader_sentiment_label'] # Assuming vader_sentiment_label is also relevant\n",
    "\n",
    "    for cluster_col in required_labels:\n",
    "        print(f\"\\n--- Analyzing Sentiment Distribution for: {cluster_col} ---\")\n",
    "\n",
    "        # Ensure the cluster column has valid data (not all None or -1)\n",
    "        if df_clean[cluster_col].isnull().all() or (cluster_col in ['dbscan_cluster_umap', 'hdbscan_cluster_umap'] and (df_clean[cluster_col] == -1).all()):\n",
    "            print(f\"  Skipping {cluster_col}: No valid clusters found or column is all null/noise.\")\n",
    "            continue\n",
    "\n",
    "        # Iterate through each sentiment column\n",
    "        for sentiment_col in sentiment_cols:\n",
    "            print(f\"\\n  Sentiment Source: {sentiment_col}\")\n",
    "\n",
    "            # Drop rows with missing values in either cluster or sentiment column for accurate counts\n",
    "            df_temp = df_clean.dropna(subset=[cluster_col, sentiment_col]).copy()\n",
    "\n",
    "            if df_temp.empty:\n",
    "                print(f\"    No valid data for {sentiment_col} and {cluster_col}.\")\n",
    "                continue\n",
    "\n",
    "            # Group by cluster and sentiment, then count occurrences\n",
    "            sentiment_by_cluster = df_temp.groupby([cluster_col, sentiment_col]).size().unstack(fill_value=0)\n",
    "\n",
    "            # Calculate percentages within each cluster\n",
    "            sentiment_by_cluster_percent = sentiment_by_cluster.divide(sentiment_by_cluster.sum(axis=1), axis=0) * 100\n",
    "\n",
    "            print(\"\\n    Count Distribution:\")\n",
    "            display(sentiment_by_cluster)\n",
    "\n",
    "            print(\"\\n    Percentage Distribution within Clusters:\")\n",
    "            display(sentiment_by_cluster_percent.round(2))\n",
    "\n",
    "            # Optional: Visualize the sentiment distribution within clusters\n",
    "            try:\n",
    "                # Melt the percentage DataFrame for plotting\n",
    "                df_melted = sentiment_by_cluster_percent.reset_index().melt(\n",
    "                    id_vars=cluster_col, var_name='Sentiment', value_name='Percentage'\n",
    "                )\n",
    "\n",
    "                plt.figure(figsize=(12, 7))\n",
    "                sns.barplot(data=df_melted, x=cluster_col, y='Percentage', hue='Sentiment', palette='viridis')\n",
    "                plt.title(f'Sentiment Distribution by Cluster ({cluster_col} - {sentiment_col})', fontsize=14)\n",
    "                plt.xlabel(f'{cluster_col}', fontsize=12)\n",
    "                plt.ylabel('Percentage within Cluster', fontsize=12)\n",
    "                plt.xticks(rotation=0)\n",
    "                plt.legend(title='Sentiment')\n",
    "                plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"    Could not generate plot for {cluster_col} and {sentiment_col}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PLR-OrUW_8F"
   },
   "source": [
    "# Identify Positive and Negative Clusters (DBSCAN UMAP):\n",
    "    *   Identified specific cluster IDs from DBSCAN that were predominantly positive or negative based on the sentiment distribution analysis.\n",
    "    *   Insights:\n",
    "        *   Identifying these specific clusters allows for focused analysis on the text content driving those sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc89a205",
    "outputId": "e8222257-ddc4-4f2f-b9a5-f6aef6e86ca1"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IDENTIFY POSITIVE AND NEGATIVE CLUSTERS (DBSCAN UMAP)\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IDENTIFYING POSITIVE AND NEGATIVE CLUSTERS (DBSCAN UMAP)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure sentiment_by_cluster_percent_dbscan is available\n",
    "if 'sentiment_by_cluster_percent_dbscan' in locals():\n",
    "    # Identify clusters with high percentages of Positive or Negative sentiment\n",
    "    # You can adjust the threshold (e.g., > 70%) based on what you consider 'predominantly'\n",
    "    positive_clusters = sentiment_by_cluster_percent_dbscan[sentiment_by_cluster_percent_dbscan['Positive'] > 70].index.tolist()\n",
    "    negative_clusters = sentiment_by_cluster_percent_dbscan[sentiment_by_cluster_percent_dbscan['Negative'] > 70].index.tolist()\n",
    "\n",
    "    print(f\"Clusters with predominantly Positive sentiment (>70%): {positive_clusters}\")\n",
    "    print(f\"Clusters with predominantly Negative sentiment (>70%): {negative_clusters}\")\n",
    "\n",
    "    # Store the identified clusters for the next steps\n",
    "    # Assuming we'll pick the first predominantly positive/negative cluster if multiple exist\n",
    "    selected_positive_cluster = positive_clusters[0] if positive_clusters else None\n",
    "    selected_negative_cluster = negative_clusters[0] if negative_clusters else None\n",
    "\n",
    "    print(\"\\nSelected clusters for Word Clouds:\")\n",
    "    print(f\"  Positive Cluster ID: {selected_positive_cluster}\")\n",
    "    print(f\"  Negative Cluster ID: {selected_negative_cluster}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: 'sentiment_by_cluster_percent_dbscan' DataFrame not found. Please run the sentiment analysis cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjc4JXw8XLM1"
   },
   "source": [
    "\n",
    "#    **Extract Text for Selected Clusters (DBSCAN UMAP):**\n",
    "    *   Filtered the data to get the processed text for the identified positive and negative DBSCAN clusters.\n",
    "    *   Insights:\n",
    "        *   Extracting the text prepares the data for further analysis or visualization of the content within those specific sentiment-driven clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d85de641",
    "outputId": "8a9d9be7-34fd-41e6-c655-a33a3705bebc"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXTRACT TEXT FOR SELECTED CLUSTERS (DBSCAN UMAP)\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXTRACTING TEXT FOR SELECTED CLUSTERS (DBSCAN UMAP)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure df_clean and the selected cluster IDs are available\n",
    "if 'df_clean' in locals() and 'selected_positive_cluster' in locals() and 'selected_negative_cluster' in locals():\n",
    "\n",
    "    # Extract text for the selected positive cluster\n",
    "    if selected_positive_cluster is not None:\n",
    "        # Use 'text_processed' column for word clouds\n",
    "        positive_cluster_texts = df_clean[df_clean['dbscan_cluster_umap'] == selected_positive_cluster]['text_processed']\n",
    "        print(f\"✓ Extracted {len(positive_cluster_texts)} texts for Positive Cluster {selected_positive_cluster}.\")\n",
    "    else:\n",
    "        positive_cluster_texts = pd.Series([]) # Empty series if no positive cluster found\n",
    "        print(\"No predominantly Positive cluster identified or selected.\")\n",
    "\n",
    "    # Extract text for the selected negative cluster\n",
    "    if selected_negative_cluster is not None:\n",
    "        # Use 'text_processed' column for word clouds\n",
    "        negative_cluster_texts = df_clean[df_clean['dbscan_cluster_umap'] == selected_negative_cluster]['text_processed']\n",
    "        print(f\"✓ Extracted {len(negative_cluster_texts)} texts for Negative Cluster {selected_negative_cluster}.\")\n",
    "    else:\n",
    "        negative_cluster_texts = pd.Series([]) # Empty series if no negative cluster found\n",
    "        print(\"No predominantly Negative cluster identified or selected.\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Required data (df_clean or selected_cluster_ids) not found. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlB62G83XTUR"
   },
   "source": [
    "#   **Summarize Cluster Characteristics (DBSCAN UMAP):**\n",
    "    *   Summarized the sentiment distribution and provided example text snippets for the DBSCAN clusters.\n",
    "    *   Insights:\n",
    "        *   This summary provides a concise overview of what characterizes each cluster found by DBSCAN, helping to interpret the clustering results in a meaningful way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7a4c5c98",
    "outputId": "834d563d-c2be-4638-cef2-b35867ce82aa"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARIZE DBSCAN CLUSTER CHARACTERISTICS (UMAP Data)\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARIZING DBSCAN CLUSTER CHARACTERISTICS (UMAP Data)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure df_clean and dbscan_cluster_umap are available\n",
    "if 'df_clean' in locals() and 'dbscan_cluster_umap' in df_clean.columns:\n",
    "    # Drop rows with noise points (-1) for cluster analysis summary\n",
    "    df_dbscan_clusters = df_clean[df_clean['dbscan_cluster_umap'] != -1].copy()\n",
    "\n",
    "    if df_dbscan_clusters.empty:\n",
    "        print(\"No valid clusters found by DBSCAN (excluding noise). Skipping summary.\")\n",
    "    else:\n",
    "        # Analyze sentiment distribution within DBSCAN clusters\n",
    "        # Use the original 'Sentiment' for summarization\n",
    "        sentiment_by_cluster_dbscan = df_dbscan_clusters.groupby(['dbscan_cluster_umap', 'Sentiment']).size().unstack(fill_value=0)\n",
    "        sentiment_by_cluster_percent_dbscan = sentiment_by_cluster_dbscan.divide(sentiment_by_cluster_dbscan.sum(axis=1), axis=0) * 100\n",
    "\n",
    "        print(\"\\n--- Original Sentiment Distribution within DBSCAN Clusters ---\")\n",
    "        display(sentiment_by_cluster_percent_dbscan.round(2))\n",
    "\n",
    "        # Provide example text snippets for each cluster\n",
    "        print(\"\\n--- Example Text Snippets from DBSCAN Clusters ---\")\n",
    "        for cluster_id in sorted(df_dbscan_clusters['dbscan_cluster_umap'].unique()):\n",
    "            print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "            cluster_texts = df_dbscan_clusters[df_dbscan_clusters['dbscan_cluster_umap'] == cluster_id]['Text'].sample(min(3, len(df_dbscan_clusters[df_dbscan_clusters['dbscan_cluster_umap'] == cluster_id])), random_state=42) # Get up to 3 samples\n",
    "            for i, text in enumerate(cluster_texts):\n",
    "                print(f\"  {i+1}. {text}\")\n",
    "\n",
    "        # You could add analysis of other features here if relevant (e.g., Source, Location)\n",
    "        # For example:\n",
    "        # print(\"\\n--- Top Sources within DBSCAN Clusters ---\")\n",
    "        # for cluster_id in sorted(df_dbscan_clusters['dbscan_cluster_umap'].unique()):\n",
    "        #    print(f\"\\n  Cluster {cluster_id}:\")\n",
    "        #    top_sources = df_dbscan_clusters[df_dbscan_clusters['dbscan_cluster_umap'] == cluster_id]['Source'].value_counts().head(3)\n",
    "        #    display(top_sources)\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Error: Required data (df_clean or dbscan_cluster_umap column) not found. Please run relevant previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0ae8b2cf",
    "outputId": "5aca482f-e88b-4a0d-800d-37b668262e52"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GENERATE WORD CLOUDS FOR SELECTED CLUSTERS\n",
    "# ============================================================================\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING WORD CLOUDS FOR SELECTED CLUSTERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure positive_cluster_texts and negative_cluster_texts are available\n",
    "if 'positive_cluster_texts' in locals() and 'negative_cluster_texts' in locals():\n",
    "\n",
    "    # --- Generate Word Cloud for Positive Cluster ---\n",
    "    if not positive_cluster_texts.empty:\n",
    "        print(\"\\n--- Generating Word Cloud for Positive Cluster ---\")\n",
    "        # Join all text in the positive cluster into a single string\n",
    "        positive_text = \" \".join(positive_cluster_texts.astype(str))\n",
    "\n",
    "        # Generate the word cloud\n",
    "        wordcloud_positive = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(positive_text)\n",
    "\n",
    "        # Display the word cloud\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud_positive, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f'Word Cloud for Positive Cluster (ID: {selected_positive_cluster})', fontsize=14)\n",
    "        plt.show()\n",
    "        print(\"✓ Positive cluster word cloud generated.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping Positive Word Cloud: No text available for the selected positive cluster.\")\n",
    "\n",
    "\n",
    "    # --- Generate Word Cloud for Negative Cluster ---\n",
    "    if not negative_cluster_texts.empty:\n",
    "        print(\"\\n--- Generating Word Cloud for Negative Cluster ---\")\n",
    "        # Join all text in the negative cluster into a single string\n",
    "        negative_text = \" \".join(negative_cluster_texts.astype(str))\n",
    "\n",
    "        # Generate the word cloud\n",
    "        wordcloud_negative = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(negative_text)\n",
    "\n",
    "        # Display the word cloud\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f'Word Cloud for Negative Cluster (ID: {selected_negative_cluster})', fontsize=14)\n",
    "        plt.show()\n",
    "        print(\"✓ Negative cluster word cloud generated.\")\n",
    "    else:\n",
    "         print(\"\\nSkipping Negative Word Cloud: No text available for the selected negative cluster.\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Required text data (positive_cluster_texts or negative_cluster_texts) not found. Please run previous extraction cell.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
